<table>
	<tr>
		<th>ID</th>
		<th>Kernel Name</th>
		<th>Train score</th>
		<th>Validation score</th>
		<th>Test score</th>
		<th>Comments</th>
	</tr>
	<tr>
		<td>1</td>
		<td>VALIDATION + metric LGB baseline</td>
		<td>0.692898</td>
		<td>0.517036</td>
		<td>0.62900</td>
		<td>Uses early stopping</td>
	</tr>
	<tr>
		<td>2</td>
		<td>VALIDATION + metric LGB baseline</td>
		<td>0.940903</td>
		<td>0.470101</td>
		<td>0.62652</td>
		<td>NOT early stopping</td>
	</tr>
	<tr>
		<td>3</td>
		<td>VALIDATION + metric LGB baseline</td>
		<td>0.682103</td>
		<td>0.520022</td>
		<td>0.62567</td>
		<td>changed some parameters (randomly)</td>
	</tr>
	<tr>
		<td>4</td>
		<td>VALIDATION + metric LGB baseline</td>
		<td>0.762885</td>
		<td>0.412678</td>
		<td>0.43256</td>
		<td>deleted mean_asset_return from features</td>
	</tr>
	<tr>
		<td>5</td>
		<td>VALIDATION + metric LGB baseline, new parameters</td>
		<td>0.978867</td>
		<td>0.481537</td>
		<td>0.63109</td>
		<td>changed hyperparameters from script 67 kernels</td>
	</tr>
	<tr>
		<td>6</td>
		<td>SINGLE MODEL SUBMISSION (TEMPLATE)</td>
		<td></td>
		<td></td>
		<td>0.58~</td>
		<td>just a basic linear regressor</td>
	</tr>
</table>
<br>
<li>
	<ul><b>[1]</b>: This kernel is a baseline with lightLGB, the main focus was on train/validation split (using 0.8 separation) and on metric function (compute sortino), the learning rate is pretty weird tough, it spikes up to 0.51 and then it goes down and then learn gradually, not that much. Uses early stopping so the model is picking 0.51. <img src="trainings/1.png"> [TODO] why volume is not in features?</ul>
	<ul><b>[2]</b>: Same kernel as [1], differnece that didn't use early stopping. Dosn't sound super good, difference of 0.51 - 0.47 in validation results in a difference of 0.629 - 0.626 in leaderboard. Need to change hyperparameters and check if substantial differnece how is influenced</ul>
	<ul><b>[3]</b>: Same kernel as [1], changed depthb of tree. improvement of 0.01 in validation doesn't have correlation with leatherboard. Conclusion<b>with the current validation method changes in validation smaller then 0.05 are not relevant</b>: '</ul>
	<ul><b>[4]</b>: This time I deleted asset related feature like asset_mean_open, that initial spike reduced. Interesting to reflect on this from feature engineerng point of vire. This time change in validation change also on public leadearboard! Great news: <b>Changes bigger then 0.1 in validation set are relevant!! We can use this validation method to do feature engineering</b> As a referebce for later the training for this model (Without asset specific features) is as follows: <img src="trainings/2.png"></ul>
	<ul><b>[5]</b>: Not really clear what happened here. I just changed super hyper parameters cause I noticed volume features had importance 0.0 in previous model. Now volume is ok, stopping at third iteration. Validation decreased, Public test increased. <img src="trainings/3.png"></ul>

</li>
</li>
