{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c20fa6deeac9d374c98774abd90bdc76b023ee63"},"cell_type":"code","source":"(market_train_df, news_train_df) = env.get_training_data()\n\n#this is a constant needed for stacking (not hyperparameter)\nmarket_train_df = market_train_df.loc[market_train_df['time'] >= '2010-01-01 22:00:00+0000']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ef1af5a9fc2bdc1ebde967994b9bb68f4745353"},"cell_type":"markdown","source":"<h1>Basic data cleaning"},{"metadata":{"trusted":true,"_uuid":"ddaf2b1ae8836ff0d9e3474a2f974d5f4c70b570"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nfrom time import time\nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf23c5fff4e62bf66e61fc743fa96b01e975100c"},"cell_type":"code","source":"def prepare_data(market_obs_df):\n    \"\"\"\n    baseline data cleaning procedure\n    Args:\n        market_obs_df (market_train_df): from env.get_training_data()\n        \n    didn't check for N.A.s\n    \"\"\"\n    start_time = time()\n    \n    market_obs_df.reset_index(drop=True, inplace=True)\n    \n    market_obs_df['close_to_open'] =  np.abs(market_obs_df['close'] / market_obs_df['open'])\n    market_obs_df['assetName_mean_open'] = market_obs_df.groupby('assetName')['open'].transform('mean')\n    market_obs_df['assetName_mean_close'] = market_obs_df.groupby('assetName')['close'].transform('mean')\n\n    # if open price is too far from mean open price for this company, replace it. Otherwise replace close price.\n    for i, row in market_obs_df.loc[market_obs_df['close_to_open'] >= 2].iterrows():\n        if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n            market_obs_df.iloc[i,5] = row['assetName_mean_open']\n        else:\n            market_obs_df.iloc[i,4] = row['assetName_mean_close']\n\n    for i, row in market_obs_df.loc[market_obs_df['close_to_open'] <= 0.5].iterrows():\n        if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n            market_obs_df.iloc[i,5] = row['assetName_mean_open']\n        else:\n            market_obs_df.iloc[i,4] = row['assetName_mean_close']\n    \n    print(\"TIME: %.2f for cleaning data\" % (time()-start_time))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3793dba744f3219ad6baf039ed15ff3d3a57845"},"cell_type":"code","source":"# sanity check on prepare data\nX = market_train_df.copy()\nprepare_data(X)\nassert len(market_train_df.columns) != len(X.columns)\ndel X","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f01d5e5216626b68aa04fbd9daad677eef752a44"},"cell_type":"markdown","source":"TODO: if I try to run linear regressor it gives me You might need a way of handling missing values, such as pandas.DataFrame.fillna or sklearn.preprocessing.Imputer. See our Missing Values tutorial for more details."},{"metadata":{"_uuid":"bc17765462132a3c94e0dd261ff95c66a7088120"},"cell_type":"markdown","source":"-----"},{"metadata":{"_uuid":"81f71b09934367c367023d38fb7012b877242bac"},"cell_type":"markdown","source":"<h1>Models"},{"metadata":{"_uuid":"e14fcdafa930280c38db1fd3c7cf79e6785f9220"},"cell_type":"markdown","source":"<h1>Linear regressor"},{"metadata":{"trusted":true,"_uuid":"32cf31e4ffc1e56498e753636688ea3bfa39964e"},"cell_type":"code","source":"def model_lr(X, Y):\n    \"\"\"\n    this is the core model used for final predictions\n    Args:\n        X: pandas.df\n        Y: values\n    Return:\n        model, results (None)\n    \"\"\"\n    \n    prepare_data(X) \n    Y = Y.clip(Y.quantile(0.001), Y.quantile(0.999))\n    Y.reset_index(drop=True, inplace=True)\n    \n    X = X.iloc[:, (X.columns != 'assetCode') \n               & (X.columns != 'assetName') \n               & (X.columns != 'time') \n               & (X.columns != 'returnsOpenNextMktres10')\n               & (X.columns != 'period')]\n    X, Y = X.fillna(0), Y.fillna(0)\n    \n    from sklearn.linear_model import LinearRegression\n    lr = LinearRegression()\n    lr.fit(X.values, Y)\n    \n    del X, Y\n    return lr, None\n\ndef linear_regressor(lr, X_test):\n    \"\"\"simple lr\n    Args:\n        X: pandas.df\n        Y: values\n        X_test: not values\n    Return:\n        predictions\n    \"\"\"\n    \n    prepare_data(X_test) \n    \n    X_test = X_test.iloc[:, (X_test.columns != 'assetCode') \n               & (X_test.columns != 'assetName') \n               & (X_test.columns != 'time') \n               & (X_test.columns != 'returnsOpenNextMktres10')\n               & (X_test.columns != 'period')]\n    X_test = X_test.fillna(0) #fillna must be in values?\n    res = lr.predict(X_test.values).clip(-1, 1)\n    \n    del X_test\n    return res","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"906e460ff2a95834316fcced2aa210a8339735bf"},"cell_type":"markdown","source":"<h2>Example\n<p>This is how to use the model for stacking, with linear_regressor"},{"metadata":{"trusted":true,"_uuid":"ade1506294761606c69d6c4176c6d42d6599004a"},"cell_type":"code","source":"X, Y = market_train_df.drop('returnsOpenNextMktres10',axis=1), market_train_df['returnsOpenNextMktres10']\nsplit = int(len(X) * 0.8)\nX_train, Y_train = X[:split], Y[:split]\nX_test, Y_test = X[split:], Y[split:]\n\nlr, _ = model_lr(X_train.copy(), Y_train.copy())\npred_lr = linear_regressor(lr, X_test.copy()) #DEBUG: important to use copy!\n\nfrom sklearn.metrics import r2_score\nprint('Test sigma score for linreg in block %d is %f' % (0, r2_score(Y_test.values, pred_lr)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72ea1dc3e03a79c7df9646ab954ab825ec42e08a"},"cell_type":"markdown","source":"<h1>Full model predictions\n<p>This is the model to make predictions after stacking"},{"metadata":{"trusted":true,"_uuid":"8e4df2d3914d036f49012be75eba20a96d53107c"},"cell_type":"code","source":"X, Y = market_train_df, market_train_df['returnsOpenNextMktres10']\nmodel00, training_results00 = model_lr(X.copy(), Y.copy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"147fbc16d803d34a24593abc4c3a66f0459f1f12"},"cell_type":"code","source":"type(model00)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c03c1ccb16e117d7e021e8b357552b5fc1cb5010"},"cell_type":"markdown","source":"<h2>LGB_0629"},{"metadata":{"trusted":true,"_uuid":"da51ac2ab7125a3ce2cb75522af800f56b2df5a5"},"cell_type":"code","source":"def model_lgb_0629(X, Y):\n        \"\"\"\n        this is the lightLGB_0629 core model that returns model (used for real predictions)\n        Args:\n            X: features\n            Y: label\n        Returns:\n            model: lightgbm instance\n            training_results: dict with training results\n        \"\"\"\n        import lightgbm as lgb\n        start_time = time()\n        \n        prepare_data(X) \n        Y = Y.clip(Y.quantile(0.001), Y.quantile(0.999))\n        Y.reset_index(drop=True, inplace=True)\n        \n        def sigma_score(preds, valid_data):\n            \"\"\"metric definition\"\"\"\n            df_time = valid_data.params['extra_time'] # will be injected afterwards\n            labels = valid_data.get_label()\n\n        #    assert len(labels) == len(df_time)\n\n            x_t = preds * labels #  * df_valid['universe'] -> Here we take out the 'universe' term because we already keep only those equals to 1.\n\n            # Here we take advantage of the fact that `labels` (used to calculate `x_t`)\n            # is a pd.Series and call `group_by`\n            x_t_sum = x_t.groupby(df_time).sum()\n            score = x_t_sum.mean() / x_t_sum.std()\n\n            return 'sigma_score', score, True\n        \n        #split train validation\n        split = int(len(X) * 0.8)\n        test_train_distsance = 2000\n        X_train, X_val = X[:split - test_train_distsance], X[split:]\n        Y_train, Y_val = Y[:split - test_train_distsance], Y[split:]\n\n        #take out universe = 0 from validation\n        universe_filter = X['universe'][split:] == 1.0\n        X_val = X_val[universe_filter]\n        Y_val = Y_val[universe_filter]\n\n        # this is a time_val series used to calc the sigma_score later, applied split and universe filter\n        time_val = X['time'][split:][universe_filter]\n        assert len(time_val) == len(X_val)\n        time_train = X['time'][:split - test_train_distsance]\n        assert len(time_train) == len(X_train)\n\n        X_train = X_train.iloc[:, (X_train.columns != 'assetCode') \n                   & (X.columns != 'assetName') \n                   & (X.columns != 'time') \n                   & (X.columns != 'returnsOpenNextMktres10')\n                   & (X.columns != 'period')]\n\n        X_val = X_val.iloc[:, (X_val.columns != 'assetCode') \n                   & (X.columns != 'assetName') \n                   & (X.columns != 'time') \n                   & (X.columns != 'returnsOpenNextMktres10')\n                   & (X.columns != 'period')]\n\n        assert len(X_train.columns) == len(X_val.columns)\n\n        train_cols = X_train.columns.tolist()\n\n        lgb_train = lgb.Dataset(X_train.values, Y_train, feature_name=train_cols, free_raw_data=False)\n        lgb_val = lgb.Dataset(X_val.values, Y_val, feature_name=train_cols, free_raw_data=False)\n\n        lgb_train.params = {\n            'extra_time' : time_train.factorize()[0]\n        }\n        lgb_val.params = {\n            'extra_time' : time_val.factorize()[0]\n        }\n\n        x_1 = [0.19000424246380565, 2452, 212, 328, 202]\n        #this is from eda script 67\n        lgb_params = {\n                'task': 'train',\n                'boosting_type': 'gbdt',\n                'objective': 'regression_l1',\n        #         'objective': 'regression',\n                'learning_rate': x_1[0],\n                'num_leaves': x_1[1],\n                'min_data_in_leaf': x_1[2],\n        #         'num_iteration': x_1[3],\n                'num_iteration': 239,\n                'max_bin': x_1[4],\n                'verbose': 1,\n                'lambda_l1': 0.0,\n                'lambda_l2' : 1.0,\n                'metric':'None'\n        }\n\n        training_results = {}\n        model = lgb.train(lgb_params, lgb_train, num_boost_round=1000, valid_sets=(lgb_val,lgb_train), valid_names=('valid','train'), verbose_eval=25,\n                      early_stopping_rounds=20, feval=sigma_score, evals_result=training_results)\n\n        print(\"\\n\\nTIME - lgb_0629: {}\".format(time()-start_time))\n        \n        del X, Y, X_train, Y_train, X_val, Y_val\n        return model, training_results\n\n    \ndef lgb_0629(model, X_test):\n    \"\"\"\n    this is the lightLGB model that got 0.629 scores on public LB (with parameters from script 67)\n    Args:\n        lgb: lgb model\n        X_test: features (not values)\n    Returns:\n        prediction: Y_test\n    \"\"\"\n    \n    prepare_data(X_test)\n    \n    X_test = X_test.iloc[:, (X_test.columns != 'assetCode') \n               & (X_test.columns != 'assetName') \n               & (X_test.columns != 'time') \n               & (X_test.columns != 'returnsOpenNextMktres10')\n               & (X_test.columns != 'period')]\n    \n    res = model.predict(X_test.values).clip(-1, 1)\n    \n    del X_test\n    return res","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"961d2d7418f83cb8553e82b9bfd07d0ad6a017a1"},"cell_type":"markdown","source":"<h2>Example\n<p>This is how to use the model for stacking, with lgb_0629 function"},{"metadata":{"trusted":true,"_uuid":"ade1506294761606c69d6c4176c6d42d6599004a"},"cell_type":"code","source":"X, Y = market_train_df, market_train_df['returnsOpenNextMktres10']\n\nsplit = int(len(X) * 0.8)\nX_train, Y_train = X[:split], Y[:split]\nX_test, Y_test = X[split:], Y[split:]\n\nmodel, results = model_lgb_0629(X_train.copy(), Y_train.copy())\npredictions = lgb_0629(model, X_test.copy()) #DEBUG: did you use copy?\n\nfrom sklearn.metrics import r2_score\nprint('Test sigma score for lbg in block %d is %f' % (0, r2_score(Y_test.values, predictions)))\n\nplt.figure(figsize=(8,4))\nplt.plot(results['train']['sigma_score'])\nplt.plot(results['valid']['sigma_score'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe752d3debf8e7f6327b7db0c6532d08fc51bdf5"},"cell_type":"markdown","source":"<h2>Full-model predictions\n<p>This is the model to make predictions after stacking"},{"metadata":{"trusted":true,"_uuid":"2bbf825abd59dfd09fbd53cb49f980706a8f4bdd"},"cell_type":"code","source":"X, Y = market_train_df, market_train_df['returnsOpenNextMktres10']\nmodel01, training_results01 = model_lgb_0629(X.copy(), Y.copy())\n\nplt.figure(figsize=(8,4))\nplt.plot(training_results01['train']['sigma_score'])\nplt.plot(training_results01['valid']['sigma_score'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13611e86d9f8443c97f13672d291358ce0268df6"},"cell_type":"markdown","source":"<h1>First level models stacking"},{"metadata":{"_uuid":"89557676c5af8e909f51a4636983e363f7229957"},"cell_type":"markdown","source":"Validation in presence of time component\n\n**f) KFold scheme in time series\n**\nIn time-series task we usually have a fixed period of time we are asked to predict. Like day, week, month or arbitrary period with duration of T.\n\nSplit the train data into chunks of duration T. Select first M chunks.\nFit N diverse models on those M chunks and predict for the chunk M+1. Then fit those models on first M+1 chunks and predict for chunk M+2 and so on, until you hit the end. After that use all train data to fit models and get predictions for test. Now we will have meta-features for the chunks starting from number M+1 as well as meta-features for the test.\nNow we can use meta-features from first K chunks [M+1,M+2,..,M+K] to fit level 2 models and validate them on chunk M+K+1. Essentially we are back to step 1. with the lesser amount of chunks and meta-features instead of features."},{"metadata":{"_uuid":"56fe13838692be290b2ecab7f419d904b67a294c"},"cell_type":"markdown","source":"T = 6 month\n* 14 periods in Train\n* 3 periods in LB\n* 1 period in pLB\n\nBuild meta features on train periods: [8, 9, 10, 11, 12, 13]"},{"metadata":{"_uuid":"5d6c47ccb13ad2ac6fc8fca7e69029c5bc29ae17"},"cell_type":"markdown","source":"<h2>Building Trainining meta-features"},{"metadata":{"_uuid":"d0246264128cfa54fcf286fe10bdf302b8292056"},"cell_type":"markdown","source":"Let's first add periods so that data manipulation will be easier"},{"metadata":{"trusted":true,"_uuid":"b9d7d74ce63db727ae00e0d20c8479d5566184fc"},"cell_type":"code","source":"market_train_df = market_train_df.rename(columns={'returnsOpenNextMktres10':'target'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e7513c3359ff60f31f3832c976e422ba13422c8"},"cell_type":"code","source":"periods = ['2010-01-01 22:00:00+0000',\n           '2010-06-15 22:00:00+0000',\n           '2011-01-01 22:00:00+0000',\n           '2011-06-15 22:00:00+0000',\n           '2012-01-01 22:00:00+0000',\n           '2012-06-15 22:00:00+0000',\n           '2013-01-01 22:00:00+0000',\n           '2013-06-15 22:00:00+0000',\n           '2014-01-01 22:00:00+0000',\n           '2014-06-15 22:00:00+0000',\n           '2015-01-01 22:00:00+0000',\n           '2015-06-15 22:00:00+0000',\n           '2016-01-01 22:00:00+0000',\n           '2016-06-15 22:00:00+0000',\n           '2017-01-01 22:00:00+0000']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9471d84e78cc3456f5f85a7f80e6136d7c2915a1"},"cell_type":"code","source":"market_train_df['period'] = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"269c51b8434bbe8d665549625c714f5e88004a8c"},"cell_type":"code","source":"for i, period in tqdm(enumerate(periods[:-1])):\n    market_train_df.loc[(market_train_df['time'] < periods[i + 1]) & (period <= market_train_df['time']), ['period']] = i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a036c80ef9f26f64322e5581c90af9cda38e52fc"},"cell_type":"code","source":"#move period to second column\ncols = market_train_df.columns.tolist()\ncols.insert(1, cols[-1])\ncols.pop()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fe8dab98f61f335596f703b790843a2680c53c2"},"cell_type":"code","source":"X_train_level1 = market_train_df[cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9027b912314fcf79f1c743ae38c18452f38129a6"},"cell_type":"code","source":"y_train_level1 = X_train_level1['target']\nX_train_level1 = X_train_level1.drop('target',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f23a4001871f4747e49c97d80d81675b5a009d1"},"cell_type":"code","source":"#sanity check\nassert len(X_train_level1['period'].unique()) == 14","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30fe1538ea296696a4b0c64f373a0a7abebf3f03"},"cell_type":"markdown","source":"Now that we cleaned the Database let's build x_train_level2"},{"metadata":{"trusted":true,"_uuid":"fc244eae4ade9c37f2424b9dbceafcef4b15d242"},"cell_type":"code","source":"periods_level2 = X_train_level1['period'][X_train_level1['period'].isin([8, 9, 10, 11, 12, 13])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"660e9a5f218b712454db95880fa36f5b515f4c73"},"cell_type":"code","source":"y_train_level2 = y_train_level1[X_train_level1['period'].isin([8, 9, 10, 11, 12, 13])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"116422e22b36f2675ffce902ae38ad793150ec1a"},"cell_type":"code","source":"# how many level1 do we have?\nlevel1_models = 2\nX_train_level2 = np.zeros([y_train_level2.shape[0], level1_models])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a0fbdedffab9bb77ddde9b811121d5d92d4aa12","scrolled":false},"cell_type":"code","source":"# Now fill `X_train_level2` with metafeatures\nfrom sklearn.metrics import r2_score\nfor cur_block_num in tqdm([8, 9, 10, 11, 12, 13]):\n    \n    print(cur_block_num)\n    \n    '''\n        1. Split `X_train` into parts\n           Remember, that corresponding dates are stored in `dates_train` \n        2. Fit linear regression \n        3. Fit LightGBM and put predictions          \n        4. Store predictions from 2. and 3. in the right place of `X_train_level2`. \n           You can use `dates_train_level2` for it\n           Make sure the order of the meta-features is the same as in `X_test_level2`\n    '''      \n    cur_block_X = X_train_level1[X_train_level1['period'] < cur_block_num]\n    cur_block_Y = y_train_level1[X_train_level1['period'] < cur_block_num]\n    \n    cur_block_X_test = X_train_level1[X_train_level1['period'] == cur_block_num]\n    cur_block_Y_test = y_train_level1[X_train_level1['period'] == cur_block_num]\n    \n    #TODO there is NA in values\n    \n    # let's train here all the models\n    '''\n    MODEL 1\n    : linear regression\n    '''\n    block_model00, results = model_lr(cur_block_X.copy(), cur_block_Y.copy())\n    pred_lr = linear_regressor(block_model00, cur_block_X_test.copy())\n    print('Test r2 score for linreg in block %d is %f' % (cur_block_num, r2_score(cur_block_Y_test, pred_lr)))\n    \n    \n    '''\n    MODEL 2\n    lightLGB (lgb_0629)\n    using 'script 67' params\n    '''\n    block_model01, results = model_lgb_0629(cur_block_X.copy(), cur_block_Y.copy())\n    pred_lgb = lgb_0629(block_model01, cur_block_X_test.copy())\n    print('Test r2 score for lgb_0629 in block %d is %f' % (cur_block_num, r2_score(cur_block_Y_test, pred_lgb)))\n    \n    cur_block_X_train_level2 = np.c_[pred_lr, pred_lgb] \n    \n    X_train_level2[periods_level2 == cur_block_num] = cur_block_X_train_level2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83dc3a2704cd9a55530593b64633db59e3a82741","scrolled":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.scatter(X_train_level2[periods_level2 == 9][:,0].clip(-0.04, 0.04), X_train_level2[periods_level2 == 9][:,1].clip(-0.04, 0.04),alpha=0.3) \nplt.scatter(X_train_level2[periods_level2 == 8][:,0].clip(-0.04, 0.04), X_train_level2[periods_level2 == 8][:,1].clip(-0.04, 0.04),alpha=0.3) \nplt.scatter(X_train_level2[periods_level2 == 10][:,0].clip(-0.04, 0.04), X_train_level2[periods_level2 == 10][:,1].clip(-0.04, 0.04),alpha=0.3) \nplt.scatter(X_train_level2[periods_level2 == 11][:,0].clip(-0.04, 0.04), X_train_level2[periods_level2 == 11][:,1].clip(-0.04, 0.04),alpha=0.3) \nplt.scatter(X_train_level2[periods_level2 == 12][:,0].clip(-0.04, 0.04), X_train_level2[periods_level2 == 12][:,1].clip(-0.04, 0.04),alpha=0.3) \nplt.scatter(X_train_level2[periods_level2 == 13][:,0].clip(-0.04, 0.04), X_train_level2[periods_level2 == 13][:,1].clip(-0.04, 0.04),alpha=0.3) \nplt.title(\"first level predictions\")\nplt.xlabel(\"Predictions of model 0\")\nplt.ylabel(\"Predictions of model 1\")\nplt.plot([-0.04, 0.04], [-0.04, 0.04])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a56d0e6fbef913fcb7ef9d218682eec5fe8cca19"},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.plot(results['train']['sigma_score'])\nplt.plot(results['valid']['sigma_score'])\n#notably good!?","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"168ff495f22c1d45e59521df158d264d537e4a86"},"cell_type":"markdown","source":"<h1>Stacking</h1>\nnow let's move to level2, and let's build a model that stacks the predictions of model of level1"},{"metadata":{"trusted":true,"_uuid":"5a7c9fdbd4f6ddc23056bd89a084439f2edf91a3"},"cell_type":"code","source":"#sanity check on level 2 training test dimensions\nassert X_train_level2.shape[0] == np.array(y_train_level2).shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"771537c101822193bea7b0d5bd7ae30c8d356432"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlevel2_model = LinearRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c38bad35a3105ac7232eaf0a76c30c2a12a7e1fa"},"cell_type":"code","source":"level2_model.fit(X_train_level2, y_train_level2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8056b881707072c379ad2e89b9c59c3c041a2ab7"},"cell_type":"markdown","source":"## Main Loop\nLet's loop through all the days and make our random predictions.  The `days` generator (returned from `get_prediction_days`) will simply stop returning values once you've reached the end."},{"metadata":{"trusted":true,"_uuid":"ef60bc52a8a228e5a2ce18e4bd416f1f1f25aeae"},"cell_type":"code","source":"days = env.get_prediction_days()\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    \n    market_obs_df['universe'] = 1\n    \n    \"\"\"\n    MODEL 00 PREDICTIONS\n    \"\"\"\n    pred_model00 = linear_regressor(model00, market_obs_df.copy())\n    \n    \"\"\"\n    MODEL 01 PREDICTIONS\n    \"\"\"\n    pred_model01 = lgb_0629(model01, market_obs_df.copy())\n    \n    \"\"\"\n    META-MODEL PREDICTIONS\n    \"\"\"\n    features_level2 = np.c_[pred_model00, pred_model01] \n    \n    #with stacking\n    #predictions_template_df.confidenceValue = level2_model.predict(features_level2).clip(-1 , 1)\n    \n    #without stacking\n    predictions_template_df.confidenceValue = pred_model01 \n    \n    \n    env.predict(predictions_template_df)\n    \nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c8fbcca87c7f6abc53e86408417bf12ce21bb7f"},"cell_type":"markdown","source":"## **`write_submission_file`** function\n\nWrites your predictions to a CSV file (`submission.csv`) in the current working directory."},{"metadata":{"trusted":true,"_uuid":"2c8ed34ffb2c47c6e124530ec798c0b4eb01ddd5"},"cell_type":"code","source":"env.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d38aa8a67cad3f0c105db7e764ec9b805db39ceb"},"cell_type":"code","source":"# We've got a submission file!\nimport os\nprint([filename for filename in os.listdir('.') if '.csv' in filename])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f464f37885ffa763a2592e2867d74685f75be506"},"cell_type":"markdown","source":"As indicated by the helper message, calling `write_submission_file` on its own does **not** make a submission to the competition.  It merely tells the module to write the `submission.csv` file as part of the Kernel's output.  To make a submission to the competition, you'll have to **Commit** your Kernel and find the generated `submission.csv` file in that Kernel Version's Output tab (note this is _outside_ of the Kernel Editor), then click \"Submit to Competition\".  When we re-run your Kernel during Stage Two, we will run the Kernel Version (generated when you hit \"Commit\") linked to your chosen Submission."},{"metadata":{"_uuid":"2e3a267ea3149403c49ff59515a1a669ca2d1f9f"},"cell_type":"markdown","source":"## Restart the Kernel to run your code again\nIn order to combat cheating, you are only allowed to call `make_env` or iterate through `get_prediction_days` once per Kernel run.  However, while you're iterating on your model it's reasonable to try something out, change the model a bit, and try it again.  Unfortunately, if you try to simply re-run the code, or even refresh the browser page, you'll still be running on the same Kernel execution session you had been running before, and the `twosigmanews` module will still throw errors.  To get around this, you need to explicitly restart your Kernel execution session, which you can do by pressing the Restart button in the Kernel Editor's bottom Console tab:\n![Restart button](https://i.imgur.com/hudu8jF.png)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}