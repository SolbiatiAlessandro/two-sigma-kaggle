<table>
	<tr>
		<th>ID</th>
		<th>Kernel Name</th>
		<th>Train score</th>
		<th>Validation score</th>
		<th>Test score</th>
		<th>Comments</th>
	</tr>
	<tr>
		<td>1</td>
		<td>VALIDATION + metric LGB baseline</td>
		<td>0.692898</td>
		<td>0.517036</td>
		<td>0.62900</td>
		<td>Uses early stopping</td>
	</tr>
	<tr>
		<td>2</td>
		<td>VALIDATION + metric LGB baseline</td>
		<td>0.940903</td>
		<td>0.470101</td>
		<td>0.62652</td>
		<td>NOT early stopping</td>
	</tr>
	<tr>
		<td>3</td>
		<td>VALIDATION + metric LGB baseline</td>
		<td>0.682103</td>
		<td>0.520022</td>
		<td>0.62567</td>
		<td>changed some parameters (randomly)</td>
	</tr>
	<tr>
		<td>4</td>
		<td>VALIDATION + metric LGB baseline</td>
		<td>0.762885</td>
		<td>0.412678</td>
		<td>0.43256</td>
		<td>deleted mean_asset_return from features</td>
	</tr>
	<tr>
		<td>5</td>
		<td>VALIDATION + metric LGB baseline, new parameters</td>
		<td>0.978867</td>
		<td>0.481537</td>
		<td>0.63109</td>
		<td>changed hyperparameters from script 67 kernels</td>
	</tr>
	<tr>
		<td>6</td>
		<td>SINGLE MODEL SUBMISSION (TEMPLATE)</td>
		<td>na</td>
		<td>na</td>
		<td>0.58437</td>
		<td>just a basic linear regressor</td>
	</tr>
	<tr>
		<td>7</td>
		<td>model_lgbm_l1-regression</td>
		<td>na</td>
		<td>na</td>
		<td>0.554455</td>
		<td>lgbm submission template with l1</td>
	</tr>
	<tr>
		<td>8</td>
		<td>model_lgbm_binary</td>
		<td>na</td>
		<td>na</td>
		<td>0.56446</td>
		<td>lgbm submission template with binary classification</td>
	</tr>
	<tr>
		<td>9</td>
		<td>forked eda 67</td>
		<td>na</td>
		<td>na</td>
		<td>0.5926</td>
		<td>simple fork of eda 67 with split validation</td>
	</tr>
	<tr>
		<td>10</td>
		<td>forked eda 67 + cross validation</td>
		<td>na</td>
		<td>na</td>
		<td>0.67031</td>
		<td>instead of split cross validation added random</td>
	</tr>
	<tr>
		<td>11</td>
		<td>model_lgbm_baseline </td>
		<td>(l1)0.0386891</td>
		<td>(l1)0.0418056</td>
		<td>0.62235</td>
		<td>STANDARD MODEL, split validation, regression l1</td>
	</tr>
	<tr>
		<td>12</td>
		<td>model_lgbm_random_validation</td>
		<td>0.0332928</td>
		<td>0.0355084</td>
		<td>0.62704</td>
		<td>STANDARD MODEL, random validation, regression l1</td>
	</tr>
	<tr>
		<td>13</td>
		<td>model_lgbm_binary_bagged_random_validation</td>
		<td>(lloss)0.425525</td>
		<td>(lloss)0.641765</td>
		<td>0.62817</td>
		<td>STANDARD MODEL, random validation, binary_classification</td>
	</tr>
</table>
<br>
<li>
	<ul><b>[1]</b>: This kernel is a baseline with lightLGB, the main focus was on train/validation split (using 0.8 separation) and on metric function (compute sortino), the learning rate is pretty weird tough, it spikes up to 0.51 and then it goes down and then learn gradually, not that much. Uses early stopping so the model is picking 0.51. <img src="trainings/1.png"> [TODO] why volume is not in features?</ul>
	<ul><b>[2]</b>: Same kernel as [1], differnece that didn't use early stopping. Dosn't sound super good, difference of 0.51 - 0.47 in validation results in a difference of 0.629 - 0.626 in leaderboard. Need to change hyperparameters and check if substantial differnece how is influenced</ul>
	<ul><b>[3]</b>: Same kernel as [1], changed depthb of tree. improvement of 0.01 in validation doesn't have correlation with leatherboard. Conclusion<b>with the current validation method changes in validation smaller then 0.05 are not relevant</b>: '</ul>
	<ul><b>[4]</b>: This time I deleted asset related feature like asset_mean_open, that initial spike reduced. Interesting to reflect on this from feature engineerng point of vire. This time change in validation change also on public leadearboard! Great news: <b>Changes bigger then 0.1 in validation set are relevant!! We can use this validation method to do feature engineering</b> As a referebce for later the training for this model (Without asset specific features) is as follows: <img src="trainings/2.png"></ul>
	<ul><b>[5]</b>: Not really clear what happened here. I just changed super hyper parameters cause I noticed volume features had importance 0.0 in previous model. Now volume is ok, stopping at third iteration. Validation decreased, Public test increased. <img src="trainings/3.png"></ul>
	<ul><b>[6]</b>: standardized model APIs, from now on all models follow API and submissino standards. This is a basic linear regressor.</ul>
	<ul><b>[7]</b>: standardize LGBM, added a lot of feature but score unexpecedly low. This uses regression with l1 reg</ul>
	<ul><b>[8]</b>: standardize LGBM, added a lot of feature but score unexpecedly low. This uses binary classification (so predict in [0,1])</ul>
	<ul><b>[9]</b>: now that we have a standard protocol I can start exploring. I tried to fork and submit eda 67 and turns out that score is super low. Reading the comments turns out is because of validation. If you put cross validation it is said to get 0.69 (!?) how is it possible? I tried to submit kernel with cross validation and see results. </ul>
	<ul><b>[10]</b>: strange result, got a 0.1 increase in leatherboard (0.67) thanks to changing cross validation from split to random, need to check different distribution of prediction between the twos</ul>
	<ul><b>[11]</b>: first STANDARD MODEL submission working properly, the model is written from scratch by me using split validation, [36] short-term lagged features on returns + weekday, regression l1 and params1 from eda 67. Training looks pretty healthy, stopped at 13th iteration. Most important feats: lag_14_openRaw10_max, volume, and lag_14_openRaw10_min. <img src="trainings/model_lgbm_baseline.png"></ul>
	<ul><b>[12]</b>:identical model as above, just changed test train validation split. Some strange things happened. (1) Score same as before (didnt change likein 10) (2) trained for over 210 periods and didnt meet early stopping, will attach train later. Is this overfitting? <img src="trainings/model_lgbm_random_validation.png"></ul>
	<ul><b>[13]</b>:this should be a replica of eda 067 but it gets only 063, not clear why. The training is with binary logloss and goes on for more then 100 epocs and looks pretty healthy, tried with clipping or not clipping score doesnt change.<img src="trainings/model_lgbm_binary_bagged_random_validation.png"></ul>

</li>
</li>
