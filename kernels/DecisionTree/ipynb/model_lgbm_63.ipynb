{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[this is a template for submission for decision tree models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fadffcaa4012badddbe0acd50c7b57b0f65843f2"
   },
   "source": [
    "<h1>Single model submission template.\n",
    "\n",
    "<p>every model to be used must satisfy the APIs requirements of model_template.py, and must pass all the tests like test_model_template.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2afca6b204da170ff55f3037a9f1bfda05c0a7c4"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a template for the APIs of models to be used into the stacking framework.\n",
    "run with Python 3.x\n",
    "\"\"\"\n",
    "try:\n",
    "    from time import time, ctime\n",
    "    import lightgbm as lgb\n",
    "    import pandas as pd\n",
    "    from matplotlib import pyplot as plt\n",
    "    from datetime import datetime\n",
    "except:\n",
    "    exit(\"ImportError: requirements -lightgbm, -pandas, -matplotlib\")\n",
    "\n",
    "\n",
    "def sigma_score(preds, valid_data):\n",
    "    \"\"\"\n",
    "    this is a custom metric used to train the model_lgbm_baseline\n",
    "    \"\"\"\n",
    "    df_time = valid_data.params['extra_time'] # will be injected afterwards\n",
    "    labels = valid_data.get_label()\n",
    "\n",
    "    #    assert len(labels) == len(df_time)\n",
    "\n",
    "    x_t = preds * labels #  * df_valid['universe'] -> Here we take out the 'universe' term because we already keep only those equals to 1.\n",
    "\n",
    "    # Here we take advantage of the fact that `labels` (used to calculate `x_t`)\n",
    "    # is a pd.Series and call `group_by`\n",
    "    x_t_sum = x_t.groupby(df_time).sum()\n",
    "    score = x_t_sum.mean() / x_t_sum.std()\n",
    "\n",
    "    return 'sigma_score', score, True\n",
    "\n",
    "class model_lgbm():\n",
    "    \"\"\"this is a replica of the original non-standardized\n",
    "    lgbm model that get 0.63109 (id=5 in history.html)\n",
    "\n",
    "    this class is for a model (that can also be\n",
    "    a combination of bagged models)\n",
    "    The commonality of the bagged models is that\n",
    "    they share the feature generation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name             = name\n",
    "        self.model            = None\n",
    "        self.type             = lgb.Booster\n",
    "        self.training_results = None\n",
    "        print(\"\\ninit model {}\".format(self.name))\n",
    "\n",
    "    def _generate_features(self, market_data, news_data, verbose=False):\n",
    "        \"\"\"\n",
    "        GENERAL:\n",
    "        given the original market_data and news_data\n",
    "        generate new features, doesn't change original data.\n",
    "        NOTE: data cleaning and preprocessing is not here,\n",
    "        here is only feats engineering\n",
    "        \n",
    "        MODEL SPECIFIC:\n",
    "        the feats of the model are\n",
    "\n",
    "        - assetName_mean_close, assetName_mean_open, close_to_open\n",
    "        - lagged feats on periods 3,7,14\n",
    "\n",
    "        the code is not super clear since I am copy pasting\n",
    "        from the non standardized. now the goal is only\n",
    "        replicate\n",
    "\n",
    "        for full dataset takes TIME 337.8714208602905\n",
    "\n",
    "        Args:\n",
    "            [market_train_df, news_train_df]: pandas.DataFrame\n",
    "        Returns:\n",
    "            complete_features: pandas.DataFrame\n",
    "        \"\"\"\n",
    "        start_time = time()\n",
    "        if verbose: print(\"Starting features generation for model {}, {}\".format(self.name, ctime()))\n",
    "\n",
    "        complete_features = market_data.copy()\n",
    "        \n",
    "        if 'returnsOpenNextMktres10' in complete_features.columns:\n",
    "            complete_features.drop(['returnsOpenNextMktres10'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "        # [21] short-term lagged features on returns\n",
    "        return_features = ['returnsClosePrevMktres10','returnsClosePrevRaw10','open','close']\n",
    "\n",
    "        def create_lag(df_code,n_lag=[3,7,14,],shift_size=1):\n",
    "            \"\"\"internal util of eda 67\"\"\"\n",
    "            code = df_code['assetCode'].unique()\n",
    "            for col in return_features:\n",
    "                for window in n_lag:\n",
    "                    rolled = df_code[col].shift(shift_size).rolling(window=window)\n",
    "                    lag_mean = rolled.mean()\n",
    "                    lag_max = rolled.max()\n",
    "                    lag_min = rolled.min()\n",
    "                    #lag_std = rolled.std()\n",
    "                    df_code['%s_lag_%s_mean'%(col,window)] = lag_mean\n",
    "                    df_code['%s_lag_%s_max'%(col,window)] = lag_max\n",
    "                    df_code['%s_lag_%s_min'%(col,window)] = lag_min\n",
    "                    #df_code['%s_lag_%s_std'%(col,window)] = lag_std\n",
    "            return df_code.fillna(-1)\n",
    "\n",
    "        def generate_lag_features(df,n_lag = [3,7,14]):\n",
    "            \"\"\"internal util of eda 67\"\"\"\n",
    "            features = ['time', 'assetCode', 'assetName', 'volume', 'close', 'open',\n",
    "               'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n",
    "               'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n",
    "               'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n",
    "               'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n",
    "               'returnsOpenNextMktres10', 'universe']\n",
    "            \n",
    "            assetCodes = df['assetCode'].unique()\n",
    "            print(assetCodes)\n",
    "            all_df = []\n",
    "            df_codes = df.groupby('assetCode')\n",
    "            df_codes = [df_code[1][['time','assetCode']+return_features] for df_code in df_codes]\n",
    "            print('total %s df'%len(df_codes))\n",
    "            \n",
    "            #TODO: fix multiprocess\n",
    "            \"\"\"Can't use multiprocess, got:\n",
    "            AttributeError: Can't pickle local object 'model_lgbm._generate_features.<locals>.create_lag'\n",
    "\n",
    "            pool = Pool(4)\n",
    "            all_df = pool.map(create_lag, df_codes)\n",
    "            \n",
    "            new_df = pd.concat(all_df)  \n",
    "            \n",
    "            new_df.drop(return_features,axis=1,inplace=True)\n",
    "            pool.close()\n",
    "            \"\"\"\n",
    "\n",
    "            all_df = [create_lag(single_asset) for single_asset in df_codes]\n",
    "            new_df = pd.concat(all_df)\n",
    "            new_df.drop(return_features,axis=1,inplace=True)\n",
    "            return new_df\n",
    "\n",
    "        n_lag = [3,7,14]\n",
    "        self.max_lag = 14\n",
    "        new_df = generate_lag_features(complete_features,n_lag)\n",
    "        complete_features = pd.merge(complete_features,new_df,how='left',on=['time','assetCode'])\n",
    "\n",
    "\n",
    "\n",
    "        # [3] asset features\n",
    "        complete_features['close_to_open'] =  np.abs(complete_features['close'] / complete_features['open'])\n",
    "        complete_features['assetName_mean_open'] = complete_features.groupby('assetName')['open'].transform('mean')\n",
    "        complete_features['assetName_mean_close'] = complete_features.groupby('assetName')['close'].transform('mean')\n",
    "\n",
    "                \n",
    "        complete_features.drop(['time','assetCode','assetName'],axis=1,inplace=True)\n",
    "\n",
    "        def mis_impute(data):\n",
    "            \"\"\"this is a fillna util from eda 67\"\"\"\n",
    "            for i in data.columns:\n",
    "                if data[i].dtype == \"object\":\n",
    "                    data[i] = data[i].fillna(\"other\")\n",
    "                elif (data[i].dtype == \"int64\" or data[i].dtype == \"float64\"):\n",
    "                    data[i] = data[i].fillna(data[i].mean())\n",
    "                else:\n",
    "                    pass\n",
    "            return data\n",
    "\n",
    "        complete_features = mis_impute(complete_features)\n",
    "\n",
    "        if verbose: print(\"Finished features generation for model {}, TIME {}\".format(self.name, time()-start_time))\n",
    "        return complete_features\n",
    "\n",
    "    def train(self, X, Y, verbose=False):\n",
    "        \"\"\"\n",
    "        GENERAL:\n",
    "        basic method to train a model with given data\n",
    "        model will be inside self.model after training\n",
    "        \n",
    "        MODEL SPECIFIC:\n",
    "        \n",
    "        - split 0.8 train validation\n",
    "        - universe filter on validation\n",
    "        - custom metric used (sigma_scored) , \n",
    "            need to put 'metric':'None' in parameters\n",
    "        - one single lgbm with params_1 from script 67\n",
    "        \n",
    "        Args:\n",
    "            X: [market_train_df, news_train_df]\n",
    "            Y: [target]\n",
    "            verbose: (bool)\n",
    "        Returns:\n",
    "            (optional) training_results\n",
    "        \"\"\"\n",
    "        start_time = time()\n",
    "        if verbose: print(\"Starting training for model {}, {}\".format(self.name, ctime()))\n",
    "\n",
    "            \n",
    "        time_reference = X[0]['time'] #time is dropped in preprocessing, but is needed later for metrics eval\n",
    "\n",
    "        X = self._generate_features(X[0], X[1], verbose=verbose)\n",
    "        Y = Y.clip(Y.quantile(0.001), Y.quantile(0.999))\n",
    "\n",
    "        # split X in X_train and Y_val\n",
    "        split = int(len(X) * 0.8)\n",
    "        test_train_distsance = 0\n",
    "        X_train, X_val = X[:split - test_train_distsance], X[split:]\n",
    "        Y_train, Y_val = Y[:split - test_train_distsance], Y[split:]\n",
    "\n",
    "        if verbose: print(\"X_train shape {}\".format(X_train.shape))\n",
    "        if verbose: print(\"X_val shape {}\".format(X_val.shape))\n",
    "        assert X_train.shape[0] != X_val.shape[0]\n",
    "        assert X_train.shape[1] == X_val.shape[1]\n",
    "\n",
    "        # universe filtering on validation set\n",
    "        universe_filter = X['universe'][split:] == 1.0\n",
    "        X_val = X_val[universe_filter]\n",
    "        Y_val = Y_val[universe_filter]\n",
    "        \n",
    "        # this is a time_val series used to calc the sigma_score later, applied split and universe filter\n",
    "        time_val = time_reference[split:][universe_filter]\n",
    "        assert len(time_val) == len(X_val) \n",
    "        time_train = time_reference[:split - test_train_distsance]\n",
    "        assert len(time_train) == len(X_train)\n",
    "        \n",
    "        # train parameters prearation\n",
    "        train_cols = X.columns.tolist()\n",
    "        assert 'returnsOpenNextMktres10' not in train_cols \n",
    "        lgb_train = lgb.Dataset(X_train.values, Y_train, feature_name=train_cols, free_raw_data=False)\n",
    "        lgb_val = lgb.Dataset(X_val.values, Y_val, feature_name=train_cols, free_raw_data=False)\n",
    "\n",
    "        lgb_train.params = {\n",
    "            'extra_time' : time_train.factorize()[0]\n",
    "        }\n",
    "        lgb_val.params = {\n",
    "            'extra_time' : time_val.factorize()[0]\n",
    "        }\n",
    "        \n",
    "        x_1 = [0.19000424246380565, 2452, 212, 328, 202]\n",
    "        #this is from eda script 67\n",
    "        lgb_params = {\n",
    "                'task': 'train',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'regression_l1',\n",
    "        #         'objective': 'regression',\n",
    "                'learning_rate': x_1[0],\n",
    "                'num_leaves': x_1[1],\n",
    "                'min_data_in_leaf': x_1[2],\n",
    "        #         'num_iteration': x_1[3],\n",
    "                'num_iteration': 239,\n",
    "                'max_bin': x_1[4],\n",
    "                'verbose': 1,\n",
    "                'lambda_l1': 0.0,\n",
    "                'lambda_l2' : 1.0,\n",
    "                'metric':'None'\n",
    "        }\n",
    "        \n",
    "        training_results = {}\n",
    "        self.model = lgb.train(\n",
    "                lgb_params,\n",
    "                lgb_train,\n",
    "                num_boost_round=1000,\n",
    "                valid_sets=(lgb_val,lgb_train), \n",
    "                valid_names=('valid','train'), \n",
    "                verbose_eval=25,\n",
    "                early_stopping_rounds=10,\n",
    "                feval=sigma_score,\n",
    "                evals_result=training_results\n",
    "                )\n",
    "        del X, X_train, X_val\n",
    "\n",
    "        if verbose: print(\"Finished training for model {}, TIME {}\".format(self.name, time()-start_time))\n",
    "\n",
    "        self.training_results = training_results\n",
    "        return training_results \n",
    "\n",
    "\n",
    "    def predict(self, X, verbose=False, do_shap=False):\n",
    "        \"\"\"\n",
    "        given a block of X features gives prediction for everyrow\n",
    "\n",
    "        Args:\n",
    "            X: [market_train_df, news_train_df]\n",
    "            shap: perform shap analysis\n",
    "        Returns:\n",
    "            y: pandas.Series\n",
    "        \"\"\"\n",
    "        start_time = time()\n",
    "        if verbose: print(\"Starting prediction for model {}, {}\".format(self.name, ctime()))\n",
    "        if self.model is None:\n",
    "            raise \"Error: model is not trained!\"\n",
    "\n",
    "        X_test = self._generate_features(X[0], X[1], verbose=verbose)\n",
    "        if verbose: print(\"X_test shape {}\".format(X_test.shape))\n",
    "        y_test = self.model.predict(X_test)\n",
    "\n",
    "        if do_shap:\n",
    "            #import pdb;pdb.set_trace()\n",
    "            print(\"printing shap analysis..\")\n",
    "            explainer = shap.TreeExplainer(self.model)\n",
    "            shap_values = explainer.shap_values(X_test)\n",
    "            shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "\n",
    "        if verbose: print(\"Finished prediction for model {}, TIME {}\".format(self.name, time()-start_time))\n",
    "        return y_test\n",
    "\n",
    "\n",
    "    def predict_rolling(self, historical_df, prediction_length, verbose=False):\n",
    "        \"\"\"\n",
    "        predict features from X, uses historical for (lagged) feature generation\n",
    "        to be used with rolling prediciton structure from competition\n",
    "\n",
    "        Args:\n",
    "            historical_df: [market_train_df, news_train_df]\n",
    "            prediction_length: generate features on historical_df, predict only on the last rows\n",
    "        \"\"\"\n",
    "        start_time = time()\n",
    "        if verbose: print(\"Starting rolled prediction for model {}, {}\".format(self.name, ctime()))\n",
    "\n",
    "        processed_historical_df = self._generate_features(historical_df[0], historical_df[1], verbose=verbose)\n",
    "        X_test = processed_historical_df.iloc[-prediction_length:]\n",
    "        if verbose: print(\"X_test shape {}\".format(X_test.shape))\n",
    "        y_test = self.model.predict(X_test)\n",
    "\n",
    "        if verbose: print(\"Finished rolled prediction for model {}, TIME {}\".format(self.name, time()-start_time))\n",
    "        return y_test\n",
    "\n",
    "    def inspect(self, X):\n",
    "        \"\"\"\n",
    "        visualize and examine the training of the model\n",
    "        Args:\n",
    "            X: for the shap values\n",
    "\n",
    "        MODEL SPECIFIC:\n",
    "        plots training results and feature importance\n",
    "        \"\"\"\n",
    "        if not self.training_results:\n",
    "            print(\"Error: No training results available\")\n",
    "        else:\n",
    "            print(\"printing training results..\")\n",
    "            for _label, key in self.training_results.items():\n",
    "                for label, result in key.items():\n",
    "                    plt.plot(result,label=_label+\" \"+label)\n",
    "            plt.title(\"Training results\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        if not self.model:\n",
    "            print(\"Error: No model available\")\n",
    "        else:\n",
    "            print(\"printing feature importance..\")\n",
    "            f=lgb.plot_importance(self.model)\n",
    "            f.figure.set_size_inches(10, 30) \n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "83cc460d930508a9445170726057d4ef07c254c9"
   },
   "source": [
    "<h1>Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from kaggle.competitions import twosigmanews\n",
    "# You can only call make_env() once, so don't lose it!\n",
    "env = twosigmanews.make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c20fa6deeac9d374c98774abd90bdc76b023ee63"
   },
   "outputs": [],
   "source": [
    "(market_train_df, news_train_df) = env.get_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "411b70914babbfa97b11a7d72409d92de936d721"
   },
   "source": [
    "<h1>`Datacleaning and preprocessing procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea1d2106702907555a9794d4194b4a23915b5ead"
   },
   "source": [
    "datacleaning will applied to the whole dataset for every model, the only requirements is that at the end of the procedure ***NO NEW FEATURES can be added here***. They must be added inside the feature generation section of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dc6b6b18ea855a1ac6a7b406a2053cd10ebc3493"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def prepare_predictions(market_obs_df):\n",
    "    market_obs_df['close_to_open'] =  np.abs(market_obs_df['close'] / market_obs_df['open'])\n",
    "    market_obs_df['assetName_mean_open'] = market_obs_df.groupby('assetName')['open'].transform('mean')\n",
    "    market_obs_df['assetName_mean_close'] = market_obs_df.groupby('assetName')['close'].transform('mean')\n",
    "\n",
    "    # if open price is too far from mean open price for this company, replace it. Otherwise replace close price.\n",
    "    for i, row in market_obs_df.loc[market_obs_df['close_to_open'] >= 2].iterrows():\n",
    "        if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n",
    "            market_obs_df.iloc[i,5] = row['assetName_mean_open']\n",
    "        else:\n",
    "            market_obs_df.iloc[i,4] = row['assetName_mean_close']\n",
    "\n",
    "    for i, row in market_obs_df.loc[market_obs_df['close_to_open'] <= 0.5].iterrows():\n",
    "        if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n",
    "            market_obs_df.iloc[i,5] = row['assetName_mean_open']\n",
    "        else:\n",
    "            market_obs_df.iloc[i,4] = row['assetName_mean_close']\n",
    "            \n",
    "    return market_obs_df.drop(['assetName_mean_open', 'assetName_mean_close','close_to_open'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2531071b01eae4a4451eb5c9a8a494adebdeaa05"
   },
   "outputs": [],
   "source": [
    "market_train_df = prepare_predictions(market_train_df)\n",
    "market_train_df = market_train_df.loc[market_train_df['time'] >= '2010-01-01 22:00:00+0000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b9929444dbe2c16e94c69806bc996e3a2e8780ae"
   },
   "outputs": [],
   "source": [
    "bottom, top = market_train_df.returnsOpenNextMktres10.quantile(0.001), market_train_df.returnsOpenNextMktres10.quantile(0.999)\n",
    "market_train_df.returnsOpenNextMktres10 = market_train_df.returnsOpenNextMktres10.clip(bottom, top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9c921f8243bbcb70223cb5d931d19a2c6191e5a5"
   },
   "source": [
    "<h1>Initialize and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b552b6f1fbac2a1e78fdafd0612c08ce8684357b"
   },
   "outputs": [],
   "source": [
    "model = model_lgbm_baseline('lgbm_baseline+logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "165a4b006519272bd71db59e430782e6e48eaabd"
   },
   "outputs": [],
   "source": [
    "target = market_train_df.returnsOpenNextMktres10\n",
    "market_train_df.drop('returnsOpenNextMktres10', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0277beff74324c769a7ea4d7ce59553873204f86"
   },
   "outputs": [],
   "source": [
    "model.train([market_train_df, news_train_df], target, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c102bb7bdc0a399341d588039a62449d01b76391"
   },
   "outputs": [],
   "source": [
    "model.inspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33a05f3a4f3d6e20ca59fc49e4c2c3b3029254bd"
   },
   "source": [
    "<h1>Prediction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "32bf42d86ee9078a6810cd07fcd5a353ccfe40f7"
   },
   "outputs": [],
   "source": [
    "days = env.get_prediction_days()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "57d93926e20010685ff3e708258b04b6d529a2ea"
   },
   "outputs": [],
   "source": [
    "# skip a prediction (for testing)\n",
    "#env.predict(predictions_template_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ef60bc52a8a228e5a2ce18e4bd416f1f1f25aeae"
   },
   "outputs": [],
   "source": [
    "from time import time, ctime\n",
    "\n",
    "total_market_df = pd.DataFrame(columns=['time', 'assetCode', 'assetName', 'volume', 'close', 'open',\n",
    "       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n",
    "       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n",
    "       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n",
    "       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10','period', 'universe'])\n",
    "\n",
    "max_lag, days_count = model.max_lag, 0\n",
    "for (market_obs_df, news_obs_df, predictions_template_df) in days:\n",
    "    days_count += 1\n",
    "    if not days_count%50: print(days_count)\n",
    "        \n",
    "    market_obs_df['period']   = days_count\n",
    "    market_obs_df['universe'] = 1\n",
    "    \n",
    "    start_time = time()\n",
    "    total_market_df = pd.concat([total_market_df, market_obs_df])\n",
    "    #total_news_obs_df.append(news_obs_df)\n",
    "        \n",
    "    history_market_df = total_market_df[total_market_df['period'] > days_count - max_lag - 1].drop('period', axis=1)\n",
    "    predictions = model.predict_rolling([history_market_df, None],\n",
    "                                        len(predictions_template_df), verbose=True)\n",
    "    \n",
    "    predictions_template_df.confidenceValue = (predictions - 0.5).clip(-1, 1)\n",
    "    env.predict(predictions_template_df)\n",
    "    print(\"[{}] loop prediction {}, TIME {}\".format(days_count, ctime(), time()-start_time))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c8fbcca87c7f6abc53e86408417bf12ce21bb7f"
   },
   "source": [
    "## **`write_submission_file`** function\n",
    "\n",
    "Writes your predictions to a CSV file (`submission.csv`) in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c8ed34ffb2c47c6e124530ec798c0b4eb01ddd5"
   },
   "outputs": [],
   "source": [
    "env.write_submission_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d38aa8a67cad3f0c105db7e764ec9b805db39ceb"
   },
   "outputs": [],
   "source": [
    "# We've got a submission file!\n",
    "import os\n",
    "print([filename for filename in os.listdir('.') if '.csv' in filename])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f464f37885ffa763a2592e2867d74685f75be506"
   },
   "source": [
    "As indicated by the helper message, calling `write_submission_file` on its own does **not** make a submission to the competition.  It merely tells the module to write the `submission.csv` file as part of the Kernel's output.  To make a submission to the competition, you'll have to **Commit** your Kernel and find the generated `submission.csv` file in that Kernel Version's Output tab (note this is _outside_ of the Kernel Editor), then click \"Submit to Competition\".  When we re-run your Kernel during Stage Two, we will run the Kernel Version (generated when you hit \"Commit\") linked to your chosen Submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e3a267ea3149403c49ff59515a1a669ca2d1f9f"
   },
   "source": [
    "## Restart the Kernel to run your code again\n",
    "In order to combat cheating, you are only allowed to call `make_env` or iterate through `get_prediction_days` once per Kernel run.  However, while you're iterating on your model it's reasonable to try something out, change the model a bit, and try it again.  Unfortunately, if you try to simply re-run the code, or even refresh the browser page, you'll still be running on the same Kernel execution session you had been running before, and the `twosigmanews` module will still throw errors.  To get around this, you need to explicitly restart your Kernel execution session, which you can do by pressing the Restart button in the Kernel Editor's bottom Console tab:\n",
    "![Restart button](https://i.imgur.com/hudu8jF.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
