{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\tVALIDATION + metric LGB baseline, new parameters\t0.978867\t0.481537\t0.63109\tchanged hyperparameters from script 67 kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from kaggle.competitions import twosigmanews\n",
    "# You can only call make_env() once, so don't lose it!\n",
    "env = twosigmanews.make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c20fa6deeac9d374c98774abd90bdc76b023ee63"
   },
   "outputs": [],
   "source": [
    "(market_train_df, news_train_df) = env.get_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3ef1af5a9fc2bdc1ebde967994b9bb68f4745353"
   },
   "source": [
    "<h1>Basic data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ddaf2b1ae8836ff0d9e3474a2f974d5f4c70b570"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9dc666f230afb14fff917986450d992644301434"
   },
   "outputs": [],
   "source": [
    "def clean_data(market_train_df):\n",
    "    \"\"\"clean data procedure\n",
    "    \n",
    "    Args:\n",
    "        market_train_df: pandas.DataFrame\n",
    "    \"\"\"\n",
    "    market_train_df['close_to_open'] =  np.abs(market_train_df['close'] / market_train_df['open'])\n",
    "    market_train_df['assetName_mean_open'] = market_train_df.groupby('assetName')['open'].transform('mean')\n",
    "    market_train_df['assetName_mean_close'] = market_train_df.groupby('assetName')['close'].transform('mean')\n",
    "    \n",
    "\n",
    "    # if open price is too far from mean open price for this company, replace it. Otherwise replace close price.\n",
    "    for i, row in market_train_df.loc[market_train_df['close_to_open'] >= 2].iterrows():\n",
    "        if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n",
    "            market_train_df.iloc[i,5] = row['assetName_mean_open']\n",
    "        else:\n",
    "            market_train_df.iloc[i,4] = row['assetName_mean_close']\n",
    "\n",
    "    for i, row in market_train_df.loc[market_train_df['close_to_open'] <= 0.5].iterrows():\n",
    "        if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n",
    "            market_train_df.iloc[i,5] = row['assetName_mean_open']\n",
    "        else:\n",
    "            market_train_df.iloc[i,4] = row['assetName_mean_close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d984af3ba4a9946681dca52c48614c6b53eda51"
   },
   "outputs": [],
   "source": [
    "clean_data(market_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9ba2dd94ee767d1e34c3380a8646bb3cf9fba3d4"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "market_train_df['time'] = market_train_df['time'].dt.date\n",
    "market_train_df = market_train_df.loc[market_train_df['time']>=date(2010, 1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "84b5a58f67ebded82e6aabc66ca36411e6db35a9"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "bottom, top = market_train_df.returnsOpenNextMktres10.quantile(0.001), market_train_df.returnsOpenNextMktres10.quantile(0.999)\n",
    "returns, binwidth = market_train_df.returnsOpenNextMktres10.clip(bottom, top), 0.005\n",
    "market_train_df.returnsOpenNextMktres10 = market_train_df.returnsOpenNextMktres10.clip(bottom, top)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.hist(returns,  bins=np.arange(min(returns), max(returns) + binwidth, binwidth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f66b2deeef82de2de4a3746c10f6be44fccc52ec"
   },
   "source": [
    "<h1>FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b29ab16d9203143580af519db2337c7d02e0187"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "return_features = ['returnsClosePrevMktres10','returnsClosePrevRaw10','open','close']\n",
    "\n",
    "def create_lag(df_code,n_lag=[3,7,14,],shift_size=1):\n",
    "    code = df_code['assetCode'].unique()\n",
    "    for col in return_features:\n",
    "        for window in n_lag:\n",
    "            rolled = df_code[col].shift(shift_size).rolling(window=window)\n",
    "            lag_mean = rolled.mean()\n",
    "            lag_max = rolled.max()\n",
    "            lag_min = rolled.min()\n",
    "            lag_std = rolled.std()\n",
    "            df_code['%s_lag_%s_mean'%(col,window)] = lag_mean\n",
    "            df_code['%s_lag_%s_max'%(col,window)] = lag_max\n",
    "            df_code['%s_lag_%s_min'%(col,window)] = lag_min\n",
    "#             df_code['%s_lag_%s_std'%(col,window)] = lag_std\n",
    "    return df_code.fillna(-1)\n",
    "\n",
    "def generate_lag_features(df,n_lag = [3,7,14]):\n",
    "    features = ['time', 'assetCode', 'assetName', 'volume', 'close', 'open',\n",
    "       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n",
    "       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n",
    "       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n",
    "       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n",
    "       'returnsOpenNextMktres10', 'universe']\n",
    "    \n",
    "    assetCodes = df['assetCode'].unique()\n",
    "    print(assetCodes)\n",
    "    all_df = []\n",
    "    df_codes = df.groupby('assetCode')\n",
    "    df_codes = [df_code[1][['time','assetCode']+return_features] for df_code in df_codes]\n",
    "    print('total %s df'%len(df_codes))\n",
    "    \n",
    "    pool = Pool(4)\n",
    "    all_df = pool.map(create_lag, df_codes)\n",
    "    \n",
    "    new_df = pd.concat(all_df)  \n",
    "    \n",
    "    new_df.drop(return_features,axis=1,inplace=True)\n",
    "    pool.close()\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "19c23327a0bdab77d9a5b17534f19076eba22693"
   },
   "outputs": [],
   "source": [
    "n_lag = [3,7,14] #leave it global\n",
    "def feature_engineering(market_train_df):\n",
    "    \"\"\"feature engineering procedure\n",
    "    Args:\n",
    "        market_train_df: Pandas.DataFrame\n",
    "    Return:\n",
    "        market_train_df\n",
    "        \n",
    "    Usage:\n",
    "        >>> market_train_df = feature_engineering(market_train_df)\n",
    "    \"\"\"\n",
    "    new_df = generate_lag_features(market_train_df,n_lag)\n",
    "    market_train_df = pd.merge(market_train_df,new_df,how='left',on=['time','assetCode'])\n",
    "    return market_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2033a5d6cf7284b1a95230bcc8fc4f4ac15480c"
   },
   "outputs": [],
   "source": [
    "market_train_df = feature_engineering(market_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e0c04a0909f845a4c64389624a1a436fc69a2419"
   },
   "source": [
    "we just created rolling averages, min (supports) and max (resistance) for ['returnsClosePrevMktres10','returnsClosePrevRaw10','open','close']. Let's visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4fd43e7efd4531d6b87a8f453f630b1043ffc062"
   },
   "outputs": [],
   "source": [
    "market_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fc6ce1e3f18ffd8efea56ff7a13106a17e19b8ef"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.title(\"price vs lag_14_min\")\n",
    "plt.plot(market_train_df[market_train_df['assetCode'] == 'AAPL.O'][-300:]['open'])\n",
    "plt.plot(market_train_df[market_train_df['assetCode'] == 'AAPL.O'][-300:]['open_lag_14_min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2f76a79d11ef2e412d3bad89f39b982a47d8f186"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.title(\"price vs lag_14_mean\")\n",
    "plt.plot(market_train_df[market_train_df['assetCode'] == 'AAPL.O'][-300:]['open'])\n",
    "plt.plot(market_train_df[market_train_df['assetCode'] == 'AAPL.O'][-300:]['open_lag_14_mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cae1301db99119a6389b3b97392ee815b7e329c4"
   },
   "source": [
    "**IDEAS:**\n",
    "* binary feature: did price cross mean\n",
    "* not only min, but try to get all support resistance levels (top 3 mins) especially in a long time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5e835ea6c27cd45494d6a52d8c37481297419351"
   },
   "source": [
    "<h1>Test-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8a70fc5ed68b41a149f72667da7d0cf8a25e5809"
   },
   "outputs": [],
   "source": [
    "X, Y = market_train_df.iloc[:, (market_train_df.columns != 'assetCode') & (market_train_df.columns != 'assetName') &(market_train_df.columns != 'time') & (market_train_df.columns != 'returnsOpenNextMktres10')], market_train_df['returnsOpenNextMktres10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b947762ed96023f9ca033ed2a1097a07677513d1"
   },
   "outputs": [],
   "source": [
    "split = int(len(X) * 0.8)\n",
    "test_train_distsance = 20000\n",
    "X_train, X_val = X[:split - test_train_distsance], X[split:]\n",
    "Y_train, Y_val = Y[:split - test_train_distsance], Y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ecc3124e7d6ef82c2a5e10f07bf415d6024c19d4"
   },
   "outputs": [],
   "source": [
    "print(len(X_val), len(Y_val))\n",
    "universe_filter = market_train_df['universe'][split:] == 1.0\n",
    "X_val = X_val[universe_filter]\n",
    "Y_val = Y_val[universe_filter]\n",
    "print(len(X_val), len(Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b60b7e9bb1f29e6ce88d2286aa889dfe1c00a3ab"
   },
   "outputs": [],
   "source": [
    "# this is a time_val series used to calc the sigma_score later, applied split and universe filter\n",
    "time_val = market_train_df['time'][split:][universe_filter]\n",
    "assert len(time_val) == len(X_val)\n",
    "time_train = market_train_df['time'][:split - test_train_distsance]\n",
    "assert len(time_train) == len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c777480aeb27f85862e6e74ddccd52163a6a988c"
   },
   "source": [
    "<h1>Metric Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4be9bbbd9f8f656dfb627564167276c331fe08cf"
   },
   "outputs": [],
   "source": [
    "def sigma_score(preds, valid_data):\n",
    "    df_time = valid_data.params['extra_time'] # will be injected afterwards\n",
    "    labels = valid_data.get_label()\n",
    "    \n",
    "#    assert len(labels) == len(df_time)\n",
    "\n",
    "    x_t = preds * labels #  * df_valid['universe'] -> Here we take out the 'universe' term because we already keep only those equals to 1.\n",
    "    \n",
    "    # Here we take advantage of the fact that `labels` (used to calculate `x_t`)\n",
    "    # is a pd.Series and call `group_by`\n",
    "    x_t_sum = x_t.groupby(df_time).sum()\n",
    "    score = x_t_sum.mean() / x_t_sum.std()\n",
    "\n",
    "    return 'sigma_score', score, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8b26a9fad87a29174ad23bc7290e51da6ab68f51"
   },
   "source": [
    "<h1>Fit basic LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a420ee62c404625c9ce41f8eb048d2cafcedac0"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a344616aa9b08f8fc5fafb48573c8954fcbb65d"
   },
   "outputs": [],
   "source": [
    "train_cols = X.columns.tolist()\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train.values, Y_train, feature_name=train_cols, free_raw_data=False)\n",
    "lgb_val = lgb.Dataset(X_val.values, Y_val, feature_name=train_cols, free_raw_data=False)\n",
    "\n",
    "lgb_train.params = {\n",
    "    'extra_time' : time_train.factorize()[0]\n",
    "}\n",
    "lgb_val.params = {\n",
    "    'extra_time' : time_val.factorize()[0]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "38a84acbf9adc645bbe4d3dbb9c240f5a29e1690"
   },
   "outputs": [],
   "source": [
    "#this get score 0.629 but volume and other feats goes to 0.0\n",
    "lgb_params_old = dict(\n",
    "    objective = 'regression_l1',\n",
    "    learning_rate = 0.1,\n",
    "    num_leaves = 127,\n",
    "    max_depth = -1,\n",
    "#     min_data_in_leaf = 1000,\n",
    "#     min_sum_hessian_in_leaf = 10,\n",
    "    bagging_fraction = 0.75,\n",
    "    bagging_freq = 2,\n",
    "    feature_fraction = 0.5,\n",
    "    lambda_l1 = 0.0,\n",
    "    lambda_l2 = 1.0,\n",
    "    metric = 'None', # This will ignore the loss objetive and use sigma_score instead,\n",
    "    seed = 42 # Change for better luck! :)\n",
    ")\n",
    "\n",
    "x_1 = [0.19000424246380565, 2452, 212, 328, 202]\n",
    "#this is from eda script 67\n",
    "lgb_params = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression_l1',\n",
    "#         'objective': 'regression',\n",
    "        'learning_rate': x_1[0],\n",
    "        'num_leaves': x_1[1],\n",
    "        'min_data_in_leaf': x_1[2],\n",
    "#         'num_iteration': x_1[3],\n",
    "        'num_iteration': 239,\n",
    "        'max_bin': x_1[4],\n",
    "        'verbose': 1,\n",
    "        'lambda_l1': 0.0,\n",
    "        'lambda_l2' : 1.0,\n",
    "        'metric':'None'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90df77baaa079c7c2083dd0855881c579173af9f"
   },
   "outputs": [],
   "source": [
    "training_results = {}\n",
    "model = lgb.train(lgb_params, lgb_train, num_boost_round=1000, valid_sets=(lgb_val,lgb_train), valid_names=('valid','train'), verbose_eval=25,\n",
    "              early_stopping_rounds=10, feval=sigma_score, evals_result=training_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5b6420cf4302cd39f499d14d6e0c780d63cf672b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(training_results['valid']['sigma_score'])\n",
    "plt.plot(training_results['train']['sigma_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ed86abeb180f3b008189ed98427f1850cb3d8a2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=lgb.plot_importance(model)\n",
    "x.figure.set_size_inches(10, 30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "26c8804f7ea473389475457a39f693a1a82abbf0"
   },
   "outputs": [],
   "source": [
    "x=lgb.plot_importance(model, importance_type='gain')\n",
    "x.figure.set_size_inches(10, 30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ae579791f6bf6b6685bfa9e5b908ace05fff30c8"
   },
   "outputs": [],
   "source": [
    "def lgbm_analyze_feats(model, col_names, top=10):\n",
    "    \"\"\"python function to print feature importances for lightgbm\n",
    "    Args:\n",
    "        model: lightgbm.basic.Booster\n",
    "        col_names: pandas.core.indexes.base.Index\n",
    "        top: int, (optional) -> e.g. print top 10 cols\n",
    "    Returns:\n",
    "        gain_sorted: list(int, string) -> gain from feat and feature name\n",
    "        split_sorted: list(int, string) -> split num and feature name\n",
    "    \"\"\"\n",
    "    gain_importances = model.feature_importance(importance_type='gain')\n",
    "    gain_sorted = sorted([(importance, col_names[i]) for i, importance in enumerate(gain_importances)], reverse=True)\n",
    "    split_importances = model.feature_importance(importance_type='split')\n",
    "    split_sorted = sorted([(importance, col_names[i]) for i, importance in enumerate(split_importances)], reverse=True)\n",
    "    print(\"\\ntop {} by gain\\n--\".format(top))\n",
    "    for i in range(top):\n",
    "        print(\"{} : {}\".format(gain_sorted[i][1], gain_sorted[i][0]))\n",
    "    print(\"\\ntop {} by split\\n--\".format(top))\n",
    "    for i in range(top):\n",
    "        print(\"{} : {}\".format(split_sorted[i][1], split_sorted[i][0]))\n",
    "    return gain_sorted, split_sorted\n",
    "_, _ = lgbm_analyze_feats(model, train_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "724c38149860c8e9058474ac9045c2301e8a20da"
   },
   "outputs": [],
   "source": [
    "# You can only iterate through a result from `get_prediction_days()` once\n",
    "# so be careful not to lose it once you start iterating.\n",
    "days = env.get_prediction_days()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8056b881707072c379ad2e89b9c59c3c041a2ab7"
   },
   "source": [
    "## Main Loop\n",
    "Let's loop through all the days and make our random predictions.  The `days` generator (returned from `get_prediction_days`) will simply stop returning values once you've reached the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "819b79a6f09aea76dcaa69ede84bbd7f4602110d"
   },
   "outputs": [],
   "source": [
    "def prepare_predictions(market_obs_df):\n",
    "    \"\"\"same procedure used for train data\"\"\"\n",
    "    clean_data(market_obs_df)\n",
    "    return feature_engineering(market_obs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eeb5b073be25eb7c32d802aeed43e48837c97a91"
   },
   "outputs": [],
   "source": [
    "def dull_predictions():\n",
    "    \"\"\"used to skip to next prediction for debugging\"\"\"\n",
    "    env.predict(predictions_template_df)\n",
    "#dull_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "599dfd1ba3eb28fa6e0b760e69d00dabae6f9768"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "\n",
    "n_days = 0\n",
    "prep_time = 0\n",
    "prediction_time = 0\n",
    "packaging_time = 0\n",
    "total_market_obs_df = []\n",
    "for (market_obs_df, news_obs_df, predictions_template_df) in days:\n",
    "    #market_obs_df, news_obs_df, predictions_template_df = next(days)\n",
    "    n_days +=1\n",
    "    if (n_days%50==0):\n",
    "        print(n_days,end=' ')\n",
    "    t = time.time()\n",
    "\n",
    "    return_features = ['returnsClosePrevMktres10','returnsClosePrevRaw10','open','close']\n",
    "    total_market_obs_df.append(market_obs_df)\n",
    "    \n",
    "    if len(total_market_obs_df)==1:\n",
    "        history_df = total_market_obs_df[0]\n",
    "    else:\n",
    "        history_df = pd.concat(total_market_obs_df[-(np.max(n_lag)+1):])\n",
    "    # we generated history_df\n",
    "\n",
    "    # apply prepare_predictions\n",
    "    new_df = prepare_predictions(history_df).drop(['assetName', 'volume', 'close', 'open',\n",
    "       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n",
    "       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n",
    "       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n",
    "       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10'], axis=1)\n",
    "    market_obs_df = pd.merge(market_obs_df,new_df,how='left',on=['time','assetCode'])\n",
    "    \n",
    "    prep_time += time.time() - t\n",
    "\n",
    "    t = time.time()\n",
    "    #predictions\n",
    "    predictions = market_obs_df.iloc[:, (market_obs_df.columns != 'assetCode') & (market_obs_df.columns != 'assetName') &(market_obs_df.columns != 'time') ]\n",
    "    predictions.insert(loc=11, column='universe', value=1.0)\n",
    "    \n",
    "    if \"close_to_open_x\" in predictions.columns:\n",
    "        # for some strange reason the three feats from data cleaning get duplicated, this is a cleaning\n",
    "        predictions = predictions.drop(['close_to_open_x','assetName_mean_open_x', 'assetName_mean_close_x'],axis=1)\n",
    "        predictions = predictions.rename(columns={'close_to_open_y':'close_to_open',\n",
    "       'assetName_mean_open_y':'assetName_mean_open', 'assetName_mean_close_y':'assetName_mean_close'})\n",
    "        \n",
    "    #and this is sanity check, prediction == train\n",
    "    assert len(predictions.columns) == len(train_cols)\n",
    "    for i, col in enumerate(predictions.columns):\n",
    "        try:\n",
    "            assert col == train_cols[i]\n",
    "        except:\n",
    "            print(col, train_cols[i])\n",
    "            print(predictions.columns, train_cols)\n",
    "    \n",
    "    predictions_template_df.confidenceValue = model.predict(predictions.values).clip(-1, 1)\n",
    "    env.predict(predictions_template_df)\n",
    "    packaging_time += time.time() - t\n",
    "    \n",
    "    print(\"preparation : {}, packaging: {}\".format(prep_time, packaging_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c8fbcca87c7f6abc53e86408417bf12ce21bb7f"
   },
   "source": [
    "# **`write_submission_file`** function\n",
    "\n",
    "Writes your predictions to a CSV file (`submission.csv`) in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c8ed34ffb2c47c6e124530ec798c0b4eb01ddd5"
   },
   "outputs": [],
   "source": [
    "env.write_submission_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d38aa8a67cad3f0c105db7e764ec9b805db39ceb"
   },
   "outputs": [],
   "source": [
    "# We've got a submission file!\n",
    "import os\n",
    "print([filename for filename in os.listdir('.') if '.csv' in filename])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f464f37885ffa763a2592e2867d74685f75be506"
   },
   "source": [
    "As indicated by the helper message, calling `write_submission_file` on its own does **not** make a submission to the competition.  It merely tells the module to write the `submission.csv` file as part of the Kernel's output.  To make a submission to the competition, you'll have to **Commit** your Kernel and find the generated `submission.csv` file in that Kernel Version's Output tab (note this is _outside_ of the Kernel Editor), then click \"Submit to Competition\".  When we re-run your Kernel during Stage Two, we will run the Kernel Version (generated when you hit \"Commit\") linked to your chosen Submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e3a267ea3149403c49ff59515a1a669ca2d1f9f"
   },
   "source": [
    "## Restart the Kernel to run your code again\n",
    "In order to combat cheating, you are only allowed to call `make_env` or iterate through `get_prediction_days` once per Kernel run.  However, while you're iterating on your model it's reasonable to try something out, change the model a bit, and try it again.  Unfortunately, if you try to simply re-run the code, or even refresh the browser page, you'll still be running on the same Kernel execution session you had been running before, and the `twosigmanews` module will still throw errors.  To get around this, you need to explicitly restart your Kernel execution session, which you can do by pressing the Restart button in the Kernel Editor's bottom Console tab:\n",
    "![Restart button](https://i.imgur.com/hudu8jF.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
