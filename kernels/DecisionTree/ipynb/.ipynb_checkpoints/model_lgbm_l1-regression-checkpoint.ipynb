{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PB: 0.554455"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fadffcaa4012badddbe0acd50c7b57b0f65843f2"
   },
   "source": [
    "<h1>Single model submission template.\n",
    "\n",
    "<p>every model to be used must satisfy the APIs requirements of model_template.py, and must pass all the tests like test_model_template.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2afca6b204da170ff55f3037a9f1bfda05c0a7c4"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a template for the APIs of models to be used into the stacking framework.\n",
    "run with Python 3.x\n",
    "\"\"\"\n",
    "try:\n",
    "    from time import time, ctime\n",
    "    import lightgbm as lgb\n",
    "    import pandas as pd\n",
    "    from matplotlib import pyplot as plt\n",
    "    from datetime import datetime\n",
    "except:\n",
    "    exit(\"ImportError: requirements -lightgbm, -pandas, -matplotlib\")\n",
    "\n",
    "\n",
    "def sigma_score(preds, valid_data):\n",
    "    \"\"\"\n",
    "    this is a custom metric used to train the model_lgbm_baseline\n",
    "    \"\"\"\n",
    "    df_time = valid_data.params['extra_time'] # will be injected afterwards\n",
    "    labels = valid_data.get_label()\n",
    "\n",
    "    #    assert len(labels) == len(df_time)\n",
    "\n",
    "    x_t = preds * labels #  * df_valid['universe'] -> Here we take out the 'universe' term because we already keep only those equals to 1.\n",
    "\n",
    "    # Here we take advantage of the fact that `labels` (used to calculate `x_t`)\n",
    "    # is a pd.Series and call `group_by`\n",
    "    x_t_sum = x_t.groupby(df_time).sum()\n",
    "    score = x_t_sum.mean() / x_t_sum.std()\n",
    "\n",
    "    return 'sigma_score', score, True\n",
    "\n",
    "class model_lgbm_baseline():\n",
    "    \"\"\"this is a baseline lightLGB model with simple features\n",
    "\n",
    "    this class is for a model (that can also be\n",
    "    a combination of bagged models)\n",
    "    The commonality of the bagged models is that\n",
    "    they share the feature generation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name             = name\n",
    "        self.model            = None\n",
    "        self.type             = lgb.Booster\n",
    "        self.training_results = None\n",
    "        print(\"\\ninit model {}\".format(self.name))\n",
    "\n",
    "    def _generate_features(self, market_data, news_data, verbose=False):\n",
    "        \"\"\"\n",
    "        GENERAL:\n",
    "        given the original market_data and news_data\n",
    "        generate new features, doesn't change original data.\n",
    "        NOTE: data cleaning and preprocessing is not here,\n",
    "        here is only feats engineering\n",
    "\n",
    "        MODEL SPECIFIC:\n",
    "        as as a baseline for decision trees model we add\n",
    "        features that are the most popular among public\n",
    "        kernels on Kaggle:\n",
    "\n",
    "        - [36] short-term lagged features on returns\n",
    "        - [6]  long-term moving averages\n",
    "        - [1]  day of the week\n",
    "\n",
    "        Args:\n",
    "            [market_train_df, news_train_df]: pandas.DataFrame\n",
    "        Returns:\n",
    "            complete_features: pandas.DataFrame\n",
    "        \"\"\"\n",
    "        start_time = time()\n",
    "        if verbose: print(\"Starting features generation for model {}, {}\".format(self.name, ctime()))\n",
    "\n",
    "        complete_features = market_data.copy()\n",
    "\n",
    "        # [36] short-term lagged features on returns\n",
    "        for feature in ['returnsClosePrevRaw10','returnsOpenPrevRaw10','returnsClosePrevMktres10','returnsOpenPrevMktres10']:\n",
    "            for lag in [3,7,14]:\n",
    "                complete_features['lag_{}_{}_max'.format(lag, feature)]  = complete_features[feature].rolling(lag, min_periods=1).max()\n",
    "                complete_features['lag_{}_{}_min'.format(lag, feature)]  = complete_features[feature].rolling(lag, min_periods=1).min()\n",
    "                complete_features['lag_{}_{}_mean'.format(lag, feature)] = complete_features[feature].rolling(lag, min_periods=1).mean()\n",
    "\n",
    "        # [6]  long-term moving averages\n",
    "        for feature in ['open','close']:\n",
    "            for lag in [50, 100, 200]:\n",
    "                complete_features['lag_{}_{}_mean'.format(lag, feature)]  = complete_features[feature].rolling(lag, min_periods=1).mean()\n",
    "\n",
    "        self.max_lag = 200\n",
    "\n",
    "        # [1]  day of the week\n",
    "        #if type(complete_features['time'][0]) == pd._libs.tslibs.timestamps.Timestamp:\n",
    "        try:    # this is Kaggle environment\n",
    "            complete_features['weekday'] = complete_features['time'].apply(lambda x: x.dayofweek)\n",
    "        except: # in test environment 'time' got to converted to str so need formatting\n",
    "            complete_features['weekday'] = complete_features['time'].apply(lambda x: datetime.strptime(x.split()[0], \"%Y-%M-%d\").weekday())\n",
    "\n",
    "\n",
    "\n",
    "        complete_features.drop(['time','assetCode','assetName'],axis=1,inplace=True)\n",
    "        complete_features.fillna(0, inplace=True) # TODO: for next models control this fillna with EDA\n",
    "\n",
    "        if verbose: print(\"Finished features generation for model {}, TIME {}\".format(self.name, time()-start_time))\n",
    "        return complete_features\n",
    "\n",
    "    def train(self, X, Y, verbose=False):\n",
    "        \"\"\"\n",
    "        GENERAL:\n",
    "        basic method to train a model with given data\n",
    "        model will be inside self.model after training\n",
    "\n",
    "        MODEL SPECIFIC:\n",
    "\n",
    "        - split 0.8 train validation\n",
    "        - universe filter on validation\n",
    "        - custom metric used (sigma_scored) ,\n",
    "            need to put 'metric':'None' in parameters\n",
    "        - one single lgbm with params_1 from script 67\n",
    "\n",
    "        Args:\n",
    "            X: [market_train_df, news_train_df]\n",
    "            Y: [target]\n",
    "            verbose: (bool)\n",
    "        Returns:\n",
    "            (optional) training_results\n",
    "        \"\"\"\n",
    "        start_time = time()\n",
    "        if verbose: print(\"Starting training for model {}, {}\".format(self.name, ctime()))\n",
    "\n",
    "        if 'returnsOpenNextMktres10' in X[0].columns:\n",
    "            X[0].drop(['returnsOpenNextMktres10'],axis=1,inplace=True)\n",
    "\n",
    "        time_reference = X[0]['time'] #time is dropped in preprocessing, but is needed later for metrics eval\n",
    "\n",
    "        X = self._generate_features(X[0], X[1], verbose=verbose)\n",
    "\n",
    "        # split X in X_train and Y_val\n",
    "        split = int(len(X) * 0.8)\n",
    "        test_train_distsance = 0\n",
    "        X_train, X_val = X[:split - test_train_distsance], X[split:]\n",
    "        Y_train, Y_val = Y[:split - test_train_distsance], Y[split:]\n",
    "\n",
    "        if verbose: print(\"X_train shape {}\".format(X_train.shape))\n",
    "        if verbose: print(\"X_val shape {}\".format(X_train.shape))\n",
    "\n",
    "        # universe filtering on validation set\n",
    "        universe_filter = X['universe'][split:] == 1.0\n",
    "        X_val = X_val[universe_filter]\n",
    "        Y_val = Y_val[universe_filter]\n",
    "\n",
    "        # this is a time_val series used to calc the sigma_score later, applied split and universe filter\n",
    "        time_val = time_reference[split:][universe_filter]\n",
    "        assert len(time_val) == len(X_val)\n",
    "        time_train = time_reference[:split - test_train_distsance]\n",
    "        assert len(time_train) == len(X_train)\n",
    "\n",
    "        # train parameters prearation\n",
    "        train_cols = X.columns.tolist()\n",
    "        assert 'returnsOpenNextMktres10' not in train_cols\n",
    "        lgb_train = lgb.Dataset(X_train.values, Y_train, feature_name=train_cols, free_raw_data=False)\n",
    "        lgb_val = lgb.Dataset(X_val.values, Y_val, feature_name=train_cols, free_raw_data=False)\n",
    "\n",
    "        lgb_train.params = {\n",
    "            'extra_time' : time_train.factorize()[0]\n",
    "        }\n",
    "        lgb_val.params = {\n",
    "            'extra_time' : time_val.factorize()[0]\n",
    "        }\n",
    "\n",
    "        x_1 = [0.19000424246380565, 2452, 212, 328, 202]\n",
    "        params_1 = {\n",
    "            # from script 67\n",
    "            'task': 'train',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression_l1',\n",
    "            # 'objective': 'regression',\n",
    "            'learning_rate': x_1[0],\n",
    "            'num_leaves': x_1[1],\n",
    "            'min_data_in_leaf': x_1[2],\n",
    "            # 'num_iteration': x_1[3],\n",
    "            'num_iteration': 239,\n",
    "            'max_bin': x_1[4],\n",
    "            'lambda_l1': 0.0,\n",
    "            'lambda_l2' : 1.0,\n",
    "            'verbose': 1\n",
    "        }\n",
    "\n",
    "\n",
    "        training_results = {}\n",
    "        # start training\n",
    "        self.model = lgb.train(\n",
    "                params_1,\n",
    "                lgb_train,\n",
    "                num_boost_round=1000,\n",
    "                valid_sets=(lgb_val,lgb_train),\n",
    "                valid_names=('valid','train'),\n",
    "                verbose_eval=25,\n",
    "                early_stopping_rounds=20,\n",
    "                #feval=sigma_score,\n",
    "                evals_result=training_results)\n",
    "        del X, X_train, X_val\n",
    "\n",
    "        if verbose: print(\"Finished training for model {}, TIME {}\".format(self.name, time()-start_time))\n",
    "\n",
    "        self.training_results = training_results\n",
    "        return training_results\n",
    "\n",
    "\n",
    "    def predict(self, X, verbose=False):\n",
    "        \"\"\"\n",
    "        given a block of X features gives prediction for everyrow\n",
    "\n",
    "        Args:\n",
    "            X: [market_train_df, news_train_df]\n",
    "        Returns:\n",
    "            y: pandas.Series\n",
    "        \"\"\"\n",
    "        start_time = time()\n",
    "        if verbose: print(\"Starting prediction for model {}, {}\".format(self.name, ctime()))\n",
    "        if self.model is None:\n",
    "            raise \"Error: model is not trained!\"\n",
    "\n",
    "        X_test = self._generate_features(X[0], X[1], verbose=verbose)\n",
    "        if verbose: print(\"X_test shape {}\".format(X_test.shape))\n",
    "        y_test = self.model.predict(X_test)\n",
    "\n",
    "        if verbose: print(\"Finished prediction for model {}, TIME {}\".format(self.name, time()-start_time))\n",
    "        return y_test\n",
    "\n",
    "\n",
    "    def predict_rolling(self, historical_df, prediction_length, verbose=False):\n",
    "        \"\"\"\n",
    "        predict features from X, uses historical for (lagged) feature generation\n",
    "        to be used with rolling prediciton structure from competition\n",
    "\n",
    "        Args:\n",
    "            historical_df: [market_train_df, news_train_df]\n",
    "            prediction_length: generate features on historical_df, predict only on the last rows\n",
    "        \"\"\"\n",
    "        start_time = time()\n",
    "        if verbose: print(\"Starting rolled prediction for model {}, {}\".format(self.name, ctime()))\n",
    "\n",
    "        processed_historical_df = self._generate_features(historical_df[0], historical_df[1], verbose=verbose)\n",
    "        X_test = processed_historical_df.iloc[-prediction_length:]\n",
    "        if verbose: print(\"X_test shape {}\".format(X_test.shape))\n",
    "        y_test = self.model.predict(X_test)\n",
    "\n",
    "        if verbose: print(\"Finished rolled prediction for model {}, TIME {}\".format(self.name, time()-start_time))\n",
    "        return y_test\n",
    "\n",
    "    def inspect(self):\n",
    "        \"\"\"\n",
    "        visualize and examine the training of the model\n",
    "\n",
    "        MODEL SPECIFIC:\n",
    "        plots training results and feature importance\n",
    "        \"\"\"\n",
    "        if not self.training_results:\n",
    "            print(\"Error: No training results available\")\n",
    "        else:\n",
    "            print(\"printing training results..\")\n",
    "            for _label, key in self.training_results.items():\n",
    "                for label, result in key.items():\n",
    "                    plt.plot(result,label=_label+\" \"+label)\n",
    "            plt.title(\"Training results\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        if not self.model:\n",
    "            print(\"Error: No model available\")\n",
    "        else:\n",
    "            print(\"printing feature importance..\")\n",
    "            f=lgb.plot_importance(self.model)\n",
    "            f.figure.set_size_inches(10, 30)\n",
    "            plt.show()\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "83cc460d930508a9445170726057d4ef07c254c9"
   },
   "source": [
    "<h1>Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from kaggle.competitions import twosigmanews\n",
    "# You can only call make_env() once, so don't lose it!\n",
    "env = twosigmanews.make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c20fa6deeac9d374c98774abd90bdc76b023ee63"
   },
   "outputs": [],
   "source": [
    "(market_train_df, news_train_df) = env.get_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "411b70914babbfa97b11a7d72409d92de936d721"
   },
   "source": [
    "<h1>`Datacleaning and preprocessing procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea1d2106702907555a9794d4194b4a23915b5ead"
   },
   "source": [
    "datacleaning will applied to the whole dataset for every model, the only requirements is that at the end of the procedure ***NO NEW FEATURES can be added here***. They must be added inside the feature generation section of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dc6b6b18ea855a1ac6a7b406a2053cd10ebc3493"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def prepare_predictions(market_obs_df):\n",
    "    market_obs_df['close_to_open'] =  np.abs(market_obs_df['close'] / market_obs_df['open'])\n",
    "    market_obs_df['assetName_mean_open'] = market_obs_df.groupby('assetName')['open'].transform('mean')\n",
    "    market_obs_df['assetName_mean_close'] = market_obs_df.groupby('assetName')['close'].transform('mean')\n",
    "\n",
    "    # if open price is too far from mean open price for this company, replace it. Otherwise replace close price.\n",
    "    for i, row in market_obs_df.loc[market_obs_df['close_to_open'] >= 2].iterrows():\n",
    "        if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n",
    "            market_obs_df.iloc[i,5] = row['assetName_mean_open']\n",
    "        else:\n",
    "            market_obs_df.iloc[i,4] = row['assetName_mean_close']\n",
    "\n",
    "    for i, row in market_obs_df.loc[market_obs_df['close_to_open'] <= 0.5].iterrows():\n",
    "        if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n",
    "            market_obs_df.iloc[i,5] = row['assetName_mean_open']\n",
    "        else:\n",
    "            market_obs_df.iloc[i,4] = row['assetName_mean_close']\n",
    "            \n",
    "    return market_obs_df.drop(['assetName_mean_open', 'assetName_mean_close','close_to_open'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2531071b01eae4a4451eb5c9a8a494adebdeaa05"
   },
   "outputs": [],
   "source": [
    "market_train_df = prepare_predictions(market_train_df)\n",
    "market_train_df = market_train_df.loc[market_train_df['time'] >= '2010-01-01 22:00:00+0000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b9929444dbe2c16e94c69806bc996e3a2e8780ae"
   },
   "outputs": [],
   "source": [
    "bottom, top = market_train_df.returnsOpenNextMktres10.quantile(0.001), market_train_df.returnsOpenNextMktres10.quantile(0.999)\n",
    "market_train_df.returnsOpenNextMktres10 = market_train_df.returnsOpenNextMktres10.clip(bottom, top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9c921f8243bbcb70223cb5d931d19a2c6191e5a5"
   },
   "source": [
    "<h1>Initialize and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b552b6f1fbac2a1e78fdafd0612c08ce8684357b"
   },
   "outputs": [],
   "source": [
    "model = model_lgbm_baseline('lgbm_baseline+logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "165a4b006519272bd71db59e430782e6e48eaabd"
   },
   "outputs": [],
   "source": [
    "target = market_train_df.returnsOpenNextMktres10\n",
    "market_train_df.drop('returnsOpenNextMktres10', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0277beff74324c769a7ea4d7ce59553873204f86"
   },
   "outputs": [],
   "source": [
    "model.train([market_train_df, news_train_df], target, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c102bb7bdc0a399341d588039a62449d01b76391"
   },
   "outputs": [],
   "source": [
    "model.inspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33a05f3a4f3d6e20ca59fc49e4c2c3b3029254bd"
   },
   "source": [
    "<h1>Prediction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "32bf42d86ee9078a6810cd07fcd5a353ccfe40f7"
   },
   "outputs": [],
   "source": [
    "days = env.get_prediction_days()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "57d93926e20010685ff3e708258b04b6d529a2ea"
   },
   "outputs": [],
   "source": [
    "# skip a prediction (for testing)\n",
    "#env.predict(predictions_template_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ef60bc52a8a228e5a2ce18e4bd416f1f1f25aeae"
   },
   "outputs": [],
   "source": [
    "from time import time, ctime\n",
    "\n",
    "total_market_df = pd.DataFrame(columns=['time', 'assetCode', 'assetName', 'volume', 'close', 'open',\n",
    "       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n",
    "       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n",
    "       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n",
    "       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10','period', 'universe'])\n",
    "\n",
    "max_lag, days_count = model.max_lag, 0\n",
    "for (market_obs_df, news_obs_df, predictions_template_df) in days:\n",
    "    days_count += 1\n",
    "    if not days_count%50: print(days_count)\n",
    "        \n",
    "    market_obs_df['period']   = days_count\n",
    "    market_obs_df['universe'] = 1\n",
    "    \n",
    "    start_time = time()\n",
    "    total_market_df = pd.concat([total_market_df, market_obs_df])\n",
    "    #total_news_obs_df.append(news_obs_df)\n",
    "        \n",
    "    history_market_df = total_market_df[total_market_df['period'] > days_count - max_lag - 1].drop('period', axis=1)\n",
    "    predictions = model.predict_rolling([history_market_df, None],\n",
    "                                        len(predictions_template_df), verbose=True)\n",
    "    \n",
    "    predictions_template_df.confidenceValue = predictions.clip(-1, 1)\n",
    "    env.predict(predictions_template_df)\n",
    "    print(\"[{}] loop prediction {}, TIME {}\".format(days_count, ctime(), time()-start_time))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c8fbcca87c7f6abc53e86408417bf12ce21bb7f"
   },
   "source": [
    "## **`write_submission_file`** function\n",
    "\n",
    "Writes your predictions to a CSV file (`submission.csv`) in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c8ed34ffb2c47c6e124530ec798c0b4eb01ddd5"
   },
   "outputs": [],
   "source": [
    "env.write_submission_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d38aa8a67cad3f0c105db7e764ec9b805db39ceb"
   },
   "outputs": [],
   "source": [
    "# We've got a submission file!\n",
    "import os\n",
    "print([filename for filename in os.listdir('.') if '.csv' in filename])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f464f37885ffa763a2592e2867d74685f75be506"
   },
   "source": [
    "As indicated by the helper message, calling `write_submission_file` on its own does **not** make a submission to the competition.  It merely tells the module to write the `submission.csv` file as part of the Kernel's output.  To make a submission to the competition, you'll have to **Commit** your Kernel and find the generated `submission.csv` file in that Kernel Version's Output tab (note this is _outside_ of the Kernel Editor), then click \"Submit to Competition\".  When we re-run your Kernel during Stage Two, we will run the Kernel Version (generated when you hit \"Commit\") linked to your chosen Submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e3a267ea3149403c49ff59515a1a669ca2d1f9f"
   },
   "source": [
    "## Restart the Kernel to run your code again\n",
    "In order to combat cheating, you are only allowed to call `make_env` or iterate through `get_prediction_days` once per Kernel run.  However, while you're iterating on your model it's reasonable to try something out, change the model a bit, and try it again.  Unfortunately, if you try to simply re-run the code, or even refresh the browser page, you'll still be running on the same Kernel execution session you had been running before, and the `twosigmanews` module will still throw errors.  To get around this, you need to explicitly restart your Kernel execution session, which you can do by pressing the Restart button in the Kernel Editor's bottom Console tab:\n",
    "![Restart button](https://i.imgur.com/hudu8jF.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
