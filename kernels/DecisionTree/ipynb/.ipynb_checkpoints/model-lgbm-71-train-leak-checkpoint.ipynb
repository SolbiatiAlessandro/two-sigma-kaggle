{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fadffcaa4012badddbe0acd50c7b57b0f65843f2"
   },
   "source": [
    "<h1>Single model submission template.\n",
    "\n",
    "<p>every model to be used must satisfy the APIs requirements of model_template.py, and must pass all the tests like test_model_template.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "2afca6b204da170ff55f3037a9f1bfda05c0a7c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/Desktop/Coding/AI/two-sigma-kaggle/env3/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is a template for the APIs of models to be used into the stacking framework.\n",
    "run with Python 3.x\n",
    "\"\"\"\n",
    "from time import time, ctime\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "from datetime import datetime, date\n",
    "import shap\n",
    "import sys\n",
    "\n",
    "\n",
    "def sigma_score(preds, valid_data):\n",
    "    \"\"\"\n",
    "    this is a custom metric used to train the model_lgbm_baseline\n",
    "    \"\"\"\n",
    "    df_time = valid_data.params['extra_time'] # will be injected afterwards\n",
    "    labels = valid_data.get_label()\n",
    "    \n",
    "    #    assert len(labels) == len(df_time)\n",
    "\n",
    "    x_t = preds * labels #  * df_valid['universe'] -> Here we take out the 'universe' term because we already keep only those equals to 1.\n",
    "    \n",
    "    # Here we take advantage of the fact that `labels` (used to calculate `x_t`)\n",
    "    # is a pd.Series and call `group_by`\n",
    "    x_t_sum = x_t.groupby(df_time).sum()\n",
    "    score = x_t_sum.mean() / x_t_sum.std()\n",
    "\n",
    "    return 'sigma_score', score, True\n",
    "\n",
    "class model():\n",
    "    \"\"\"this is a baseline lightLGB model with simple features\n",
    "\n",
    "    this class is for a model (that can also be\n",
    "    a combination of bagged models)\n",
    "    The commonality of the bagged models is that\n",
    "    they share the feature generation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name             = name\n",
    "        self.type             = lgb.Booster\n",
    "        self.model1, self.model2 = None, None\n",
    "        self.training_results = None\n",
    "        print(\"\\ninit model {}\".format(self.name))\n",
    "        sys.path.insert(0, '../') # this is for imports from /kernels\n",
    "\n",
    "    def _preprocess(self, market_data):\n",
    "        \"\"\"optional data preprocessing\"\"\"\n",
    "        try:\n",
    "            market_data = market_data.loc[market_data['time']>=date(2010, 1, 1)]\n",
    "        except TypeError: # if 'time' is a string value\n",
    "            print(\"[_generate_features] 'time' is of type str and not datetime\")\n",
    "            if not market_data.loc[market_data['time']>=\"2010\"].empty:\n",
    "                # if dates are before 2010 means dataset is for testing\n",
    "                market_data = market_data.loc[market_data['time']>=\"2010\"]\n",
    "        assert market_data.empty == False\n",
    "        return market_data\n",
    "        \n",
    "\n",
    "    def _generate_features(self, market_data, news_data, verbose=False, normalize=True):\n",
    "        \"\"\"\n",
    "        GENERAL:\n",
    "        given the original market_data and news_data\n",
    "        generate new features, doesn't change original data.\n",
    "        NOTE: data cleaning and preprocessing is not here,\n",
    "        here is only feats engineering\n",
    "        \n",
    "        MODEL SPECIFIC:\n",
    "        as as a baseline for decision trees model we add\n",
    "        features that are the most popular among public\n",
    "        kernels on Kaggle:\n",
    "        \n",
    "        - [36] short-term lagged features on returns\n",
    "        - has been removed (cant pass tests) [6]  long-term moving averages\n",
    "        - [1]  day of the week\n",
    "\n",
    "        Args:\n",
    "            [market_train_df, news_train_df]: pandas.DataFrame\n",
    "            normalize: should be True, but for testing we need to\n",
    "                 be able to toggle to check real values\n",
    "        Returns:\n",
    "            complete_features: pandas.DataFrame\n",
    "        \"\"\"\n",
    "        #from utils import progress\n",
    "        start_time = time()\n",
    "        if verbose: print(\"Starting features generation for model {}, {}\".format(self.name, ctime()))\n",
    "\n",
    "        complete_features = market_data.copy()\n",
    "\n",
    "        if 'returnsOpenNextMktres10' in complete_features.columns:\n",
    "            complete_features.drop(['returnsOpenNextMktres10'],axis=1,inplace=True)\n",
    "\n",
    "        #### [36] short-term lagged features on returns ####\n",
    "          \n",
    "\n",
    "        def create_lag(df_code,n_lag=[3,7,14,],shift_size=1):\n",
    "            code = df_code['assetCode'].unique()\n",
    "            \n",
    "            # how to print progress in preprocessing?\n",
    "            # progress(0, len(n_lag)*len(return_features), prefix = 'Lagged features generation:', length=50)\n",
    "            # print(\"\\rcreating lags for {}\".format(code))\n",
    "            for _feature, col in enumerate(return_features):\n",
    "                for _lag, window in enumerate(n_lag):\n",
    "                    rolled = df_code[col].shift(shift_size).rolling(window=window)\n",
    "                    lag_mean = rolled.mean()\n",
    "                    lag_max = rolled.max()\n",
    "                    lag_min = rolled.min()\n",
    "                    lag_std = rolled.std()\n",
    "                    df_code['lag_%s_%s_mean'%(window,col)] = lag_mean\n",
    "                    df_code['lag_%s_%s_max'%(window,col)] = lag_max\n",
    "                    df_code['lag_%s_%s_min'%(window,col)] = lag_min\n",
    "        #             df_code['%s_lag_%s_std'%(col,window)] = lag_std\n",
    "                    #progress(_feature * len(n_lag) + _lag, len(n_lag) * len(return_features), \n",
    "                    #prefix = 'Lagged features generation:', length = 50)\n",
    "            return df_code.fillna(-1)\n",
    "\n",
    "        def generate_lag_features(df,n_lag = [3,7,14]):\n",
    "            features = ['time', 'assetCode', 'assetName', 'volume', 'close', 'open',\n",
    "               'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n",
    "               'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n",
    "               'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n",
    "               'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n",
    "               'returnsOpenNextMktres10', 'universe']\n",
    "            \n",
    "            assetCodes = df['assetCode'].unique()\n",
    "            print(assetCodes)\n",
    "            all_df = []\n",
    "            df_codes = df.groupby('assetCode')\n",
    "            df_codes = [df_code[1][['time','assetCode']+return_features] for df_code in df_codes]\n",
    "            print('total %s df'%len(df_codes))\n",
    "\n",
    "            pool = Pool(4)\n",
    "            all_df = pool.map(create_lag, df_codes)\n",
    "            \n",
    "            new_df = pd.concat(all_df)  \n",
    "            new_df.drop(return_features,axis=1,inplace=True)\n",
    "            pool.close()\n",
    "\n",
    "            # for the next two lines\n",
    "            # https://stackoverflow.com/questions/49888485/pathos-multiprocessings-pool-appears-to-be-nonlocal\n",
    "            pool.terminate()\n",
    "            pool.restart()\n",
    "            \n",
    "            return new_df\n",
    "\n",
    "        return_features = ['returnsClosePrevMktres10','returnsClosePrevRaw10','open','close']\n",
    "        n_lag = [3,7,14]\n",
    "        new_df = generate_lag_features(complete_features,n_lag=n_lag)\n",
    "        new_df['time'] = pd.to_datetime(new_df['time'])\n",
    "        complete_features['time'] = pd.to_datetime(complete_features['time'])\n",
    "        complete_features = pd.merge(complete_features,new_df,how='left',on=['time','assetCode'])\n",
    "        self.max_lag = max(n_lag)\n",
    "\n",
    "        complete_features = self._clean_data(complete_features)\n",
    "\n",
    "        #### [1]  generate labels encoding for assetCode ####\n",
    "\n",
    "        def data_prep(market_train):\n",
    "            \"\"\"procedure from https://www.kaggle.com/guowenrui/sigma-eda-versionnew\n",
    "            \"\"\"\n",
    "            lbl = {k: v for v, k in enumerate(market_train['assetCode'].unique())}\n",
    "            market_train['assetCodeT'] = market_train['assetCode'].map(lbl)\n",
    "            market_train = market_train.dropna(axis=0)\n",
    "            return market_train\n",
    "\n",
    "        complete_features = data_prep(complete_features)\n",
    "\n",
    "\n",
    "        #### drop columns ####\n",
    "\n",
    "        fcol = [c for c in complete_features if c not in ['assetCode', 'assetCodes', 'assetCodesLen', 'assetName', 'audiences', \n",
    "                                                         'firstCreated', 'headline', 'headlineTag', 'marketCommentary', 'provider', \n",
    "                                                                                                      'returnsOpenNextMktres10', 'sourceId', 'subjects', 'time', 'time_x', 'universe','sourceTimestamp']]\n",
    "        complete_features = complete_features[fcol]\n",
    "\n",
    "\n",
    "        #### normalization of input ####\n",
    "\n",
    "        if normalize:\n",
    "            mins = np.min(complete_features, axis=0)\n",
    "            maxs = np.max(complete_features, axis=0)\n",
    "            rng = maxs - mins\n",
    "            complete_features = 1 - ((maxs - complete_features) / rng)\n",
    "\n",
    "        if verbose: print(\"Finished features generation for model {}, TIME {}\".format(self.name, time()-start_time))\n",
    "        return complete_features\n",
    "\n",
    "    def _generate_target(self, Y):\n",
    "        \"\"\"\n",
    "        given Y generate binary labels\n",
    "        returns:\n",
    "            up, r : (binary labels), (returns)\n",
    "        \"\"\"\n",
    "        binary_labels = Y >= 0\n",
    "        return binary_labels.astype(int).values, Y.values\n",
    "\n",
    "    def train(self, X, Y, verbose=False):\n",
    "        \"\"\"\n",
    "        GENERAL:\n",
    "        basic method to train a model with given data\n",
    "        model will be inside self.model after training\n",
    "        \n",
    "        MODEL SPECIFIC:\n",
    "        \n",
    "        - sklearn random split\n",
    "        - universe filter on validation\n",
    "        - binary classification\n",
    "            need to put 'metric':'None' in parameters\n",
    "        - target is Y > 0 \n",
    "        \n",
    "        Args:\n",
    "            X: [market_train_df, news_train_df]\n",
    "            Y: [target]\n",
    "            verbose: (bool)\n",
    "        Returns:\n",
    "            (optional) training_results\n",
    "        \"\"\"\n",
    "        start_time = time()\n",
    "        if verbose: print(\"Starting training for model {}, {}\".format(self.name, ctime()))\n",
    "            \n",
    "        time_reference = X[0]['time'] #time is dropped in preprocessing, but is needed later for metrics eval\n",
    "        universe_reference = X[0]['universe']\n",
    "\n",
    "        X = self._generate_features(X[0], X[1], verbose=verbose)\n",
    "        binary_Y, Y = self._generate_target(Y)\n",
    "\n",
    "        try:\n",
    "            assert X.shape[0] == binary_Y.shape[0] == Y.shape[0]\n",
    "        except AssertionError:\n",
    "            import pdb;pdb.set_trace()\n",
    "            pass\n",
    "\n",
    "        from sklearn import model_selection\n",
    "        X_train, X_val,\\\n",
    "        binary_Y_train, binary_Y_val,\\\n",
    "        Y_train, Y_val,\\\n",
    "        universe_train, universe_val,\\\n",
    "        time_train, time_val = model_selection.train_test_split(\n",
    "                X, \n",
    "                binary_Y,\n",
    "                Y,\n",
    "                universe_reference.values,\n",
    "                time_reference, test_size=0.25, random_state=99)\n",
    "\n",
    "        assert X_train.shape[0] == Y_train.shape[0] == binary_Y_train.shape[0]\n",
    "\n",
    "        if verbose: print(\"X_train shape {}\".format(X_train.shape))\n",
    "        if verbose: print(\"X_val shape {}\".format(X_val.shape))\n",
    "        assert X_train.shape[0] != X_val.shape[0]\n",
    "        assert X_train.shape[1] == X_val.shape[1]\n",
    "\n",
    "        # train parameters prearation\n",
    "        train_cols = X.columns.tolist()\n",
    "        assert 'returnsOpenNextMktres10' not in train_cols \n",
    "        train_data = lgb.Dataset(X.values, binary_Y, feature_name=train_cols)\n",
    "        test_data = lgb.Dataset(X_val.values, binary_Y_val, feature_name=train_cols)\n",
    "\n",
    "        x_1 = [0.19000424246380565, 2452, 212, 328, 202]\n",
    "        x_2 = [0.19016805202090095, 2583, 213, 312, 220]\n",
    "        x_3 = [0.19564034613157152, 2452, 210, 160, 219]\n",
    "        x_4 = [0.19016805202090095, 2500, 213, 150, 202]\n",
    "        x_5 = [0.19000424246380565, 2600, 215, 140, 220]\n",
    "        x_6 = [0.19000424246380565, 2652, 216, 152, 202]\n",
    "\n",
    "        params_1 = {\n",
    "                'task': 'train',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'learning_rate': x_1[0],\n",
    "                'num_leaves': x_1[1],\n",
    "                'min_data_in_leaf': x_1[2],\n",
    "                'num_iteration': 239,\n",
    "                'max_bin': x_1[4],\n",
    "                'verbose': 1\n",
    "            }\n",
    "\n",
    "        params_2 = {\n",
    "                'task': 'train',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'learning_rate': x_2[0],\n",
    "                'num_leaves': x_2[1],\n",
    "                'min_data_in_leaf': x_2[2],\n",
    "                'num_iteration': 172,\n",
    "                'max_bin': x_2[4],\n",
    "                'verbose': 1\n",
    "            }\n",
    "\n",
    "\n",
    "        params_3 = {\n",
    "                'task': 'train',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'learning_rate': x_3[0],\n",
    "                'num_leaves': x_3[1],\n",
    "                'min_data_in_leaf': x_3[2],\n",
    "                'num_iteration': x_3[3],\n",
    "                'max_bin': x_3[4],\n",
    "                'verbose': 1\n",
    "            }\n",
    "\n",
    "        params_4 = {\n",
    "                'task': 'train',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'learning_rate': x_4[0],\n",
    "                'num_leaves': x_4[1],\n",
    "                'min_data_in_leaf': x_4[2],\n",
    "                'num_iteration': x_4[3],\n",
    "                'max_bin': x_4[4],\n",
    "                'verbose': 1\n",
    "            }\n",
    "\n",
    "        params_5 = {\n",
    "                'task': 'train',\n",
    "                'boosting_type': 'gbdt',#dart\n",
    "                'objective': 'binary',\n",
    "                'learning_rate': x_5[0],\n",
    "                'num_leaves': x_5[1],\n",
    "                'min_data_in_leaf': x_5[2],\n",
    "                'num_iteration': x_5[3],\n",
    "                'max_bin': x_5[4],\n",
    "                'verbose': 1\n",
    "            }\n",
    "\n",
    "        params_6 = {\n",
    "                'task': 'train',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'learning_rate': x_6[0],\n",
    "                'num_leaves': x_6[1],\n",
    "                'min_data_in_leaf': x_6[2],\n",
    "                'num_iteration': x_6[3],\n",
    "                'max_bin': x_6[4],\n",
    "                'verbose': 1\n",
    "            }\n",
    "\n",
    "        training_results = {}\n",
    "        self.model1 = lgb.train(params_1,\n",
    "                train_data,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=(test_data, train_data),\n",
    "                valid_names=('valid','train'),\n",
    "                early_stopping_rounds=5,\n",
    "                verbose_eval=1,\n",
    "                evals_result=training_results)\n",
    "\n",
    "        self.model2 = lgb.train(params_2,\n",
    "                train_data,\n",
    "                valid_sets=(test_data, train_data),\n",
    "                valid_names=('valid','train'),\n",
    "                num_boost_round=100,\n",
    "                verbose_eval=1,\n",
    "                early_stopping_rounds=5,\n",
    "                evals_result=training_results)\n",
    "\n",
    "\n",
    "        self.model3 = lgb.train(params_3,\n",
    "                train_data,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=test_data,\n",
    "                early_stopping_rounds=5,\n",
    "        #         fobj=exp_loss,\n",
    "                )\n",
    "\n",
    "        self.model4 = lgb.train(params_4,\n",
    "                train_data,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=test_data,\n",
    "                early_stopping_rounds=5,\n",
    "        #         fobj=exp_loss,\n",
    "                )\n",
    "\n",
    "        self.model5 = lgb.train(params_5,\n",
    "                train_data,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=test_data,\n",
    "                early_stopping_rounds=5,\n",
    "        #         fobj=exp_loss,\n",
    "                )\n",
    "\n",
    "\n",
    "        self.model6 = lgb.train(params_6,\n",
    "                train_data,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=test_data,\n",
    "                early_stopping_rounds=10,\n",
    "        #         fobj=exp_loss,\n",
    "                )\n",
    "\n",
    "        del X, X_train, X_val\n",
    "\n",
    "        if verbose: print(\"Finished training for model {}, TIME {}\".format(self.name, time()-start_time))\n",
    "\n",
    "        self.training_results = training_results\n",
    "        return training_results \n",
    "\n",
    "    def predict(self, X, verbose=False, do_shap=False):\n",
    "        \"\"\"\n",
    "        given a block of X features gives prediction for everyrow\n",
    "\n",
    "        Args:\n",
    "            X: [market_train_df, news_train_df]\n",
    "            shap: perform shap analysis\n",
    "        Returns:\n",
    "            y: pandas.Series\n",
    "        \"\"\"\n",
    "        start_time = time()\n",
    "        if verbose: print(\"Starting prediction for model {}, {}\".format(self.name, ctime()))\n",
    "        if self.model1 is None or self.model2 is None:\n",
    "            raise \"Error: model is not trained!\"\n",
    "\n",
    "        X_test = self._generate_features(X[0], X[1], verbose=verbose)\n",
    "        if verbose: print(\"X_test shape {}\".format(X_test.shape))\n",
    "        preds= [self.model1.predict(X_test), self.model2.predict(X_test)]\n",
    "        preds.append(self.model3.predict(X_test))\n",
    "        preds.append(self.model4.predict(X_test))\n",
    "        preds.append(self.model5.predict(X_test))\n",
    "        preds.append(self.model6.predict(X_test))\n",
    "        y_test = self._postprocess(preds)\n",
    "\n",
    "        if do_shap:\n",
    "            #import pdb;pdb.set_trace()\n",
    "            print(\"printing shap analysis..\")\n",
    "            explainer = shap.TreeExplainer(self.model1)\n",
    "            shap_values = explainer.shap_values(X_test)\n",
    "            shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "\n",
    "        if verbose: print(\"Finished prediction for model {}, TIME {}\".format(self.name, time()-start_time))\n",
    "        return y_test\n",
    "\n",
    "    def predict_rolling(self, historical_df, prediction_length, verbose=False):\n",
    "        \"\"\"\n",
    "        predict features from X, uses historical for (lagged) feature generation\n",
    "        to be used with rolling prediciton structure from competition\n",
    "\n",
    "        Args:\n",
    "            historical_df: [market_train_df, news_train_df]\n",
    "            prediction_length: generate features on historical_df, predict only on the last rows\n",
    "        \"\"\"\n",
    "        start_time = time()\n",
    "        if verbose: print(\"Starting rolled prediction for model {}, {}\".format(self.name, ctime()))\n",
    "        if self.model1 is None or self.model2 is None:\n",
    "            raise \"Error: model is not trained!\"\n",
    "\n",
    "        processed_historical_df = self._generate_features(historical_df[0], historical_df[1], verbose=verbose)\n",
    "        X_test = processed_historical_df.iloc[-prediction_length:]\n",
    "        if verbose: print(\"X_test shape {}\".format(X_test.shape))\n",
    "        preds= [self.model1.predict(X_test), self.model2.predict(X_test)]\n",
    "        preds.append(self.model3.predict(X_test))\n",
    "        preds.append(self.model4.predict(X_test))\n",
    "        preds.append(self.model5.predict(X_test))\n",
    "        preds.append(self.model6.predict(X_test))\n",
    "        y_test = self._postprocess(preds)\n",
    "\n",
    "        if verbose: print(\"Finished rolled prediction for model {}, TIME {}\".format(self.name, time()-start_time))\n",
    "        return y_test\n",
    "\n",
    "    def inspect(self, X):\n",
    "        \"\"\"\n",
    "        visualize and examine the training of the model\n",
    "        Args:\n",
    "            X: for the shap values\n",
    "\n",
    "        MODEL SPECIFIC:\n",
    "        plots training results and feature importance\n",
    "        \"\"\"\n",
    "        if not self.training_results:\n",
    "            print(\"Error: No training results available\")\n",
    "        else:\n",
    "            print(\"printing training results..\")\n",
    "            for _label, key in self.training_results.items():\n",
    "                for label, result in key.items():\n",
    "                    plt.plot(result,label=_label+\" \"+label)\n",
    "            plt.title(\"Training results\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        if not self.model1:\n",
    "            print(\"Error: No model available\")\n",
    "        else:\n",
    "            print(\"printing feature importance..\")\n",
    "            f=lgb.plot_importance(self.model1)\n",
    "            f.figure.set_size_inches(10, 30) \n",
    "            plt.show()\n",
    "\n",
    "    def _postprocess(self, predictions):\n",
    "        \"\"\"\n",
    "        post processing of predictions\n",
    "\n",
    "        Args:\n",
    "            predictions: list(np.array) might be from\n",
    "                different models\n",
    "        Return:\n",
    "            predictions: np.array\n",
    "\n",
    "        MODEL SPECIFIC:\n",
    "        the postprocessing is needed to ensemble bagged\n",
    "        models and to map prediction interval from [0, 1] \n",
    "        to [-1, 1]\n",
    "        \"\"\"\n",
    "        y_test = sum(predictions)/len(predictions)\n",
    "        y_test = (y_test-y_test.min())/(y_test.max()-y_test.min())\n",
    "        y_test = y_test * 2 - 1\n",
    "        return y_test\n",
    "\n",
    "    def _clean_data(self, data):\n",
    "        \"\"\"\n",
    "        originally from function mis_impute in\n",
    "        https://www.kaggle.com/guowenrui/sigma-eda-versionnew\n",
    "\n",
    "        Args:\n",
    "            data: pd.DataFrame\n",
    "        returns:\n",
    "            cleaned data (not in place)\n",
    "        \"\"\"\n",
    "        for i in data.columns:\n",
    "            if data[i].dtype == \"object\":\n",
    "                    data[i] = data[i].fillna(\"other\")\n",
    "            elif (data[i].dtype == \"int64\" or data[i].dtype == \"float64\"):\n",
    "                    data[i] = data[i].fillna(data[i].mean())\n",
    "            else:\n",
    "                    pass\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "83cc460d930508a9445170726057d4ef07c254c9"
   },
   "source": [
    "<h1>Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from kaggle.competitions import twosigmanews\n",
    "# You can only call make_env() once, so don't lose it!\n",
    "env = twosigmanews.make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c20fa6deeac9d374c98774abd90bdc76b023ee63"
   },
   "outputs": [],
   "source": [
    "(market_train_df, news_train_df) = env.get_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "411b70914babbfa97b11a7d72409d92de936d721"
   },
   "source": [
    "<h1>`Datacleaning and preprocessing procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea1d2106702907555a9794d4194b4a23915b5ead"
   },
   "source": [
    "datacleaning will applied to the whole dataset for every model, the only requirements is that at the end of the procedure ***NO NEW FEATURES can be added here***. They must be added inside the feature generation section of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dc6b6b18ea855a1ac6a7b406a2053cd10ebc3493"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2531071b01eae4a4451eb5c9a8a494adebdeaa05"
   },
   "outputs": [],
   "source": [
    "market_train_df = market_train_df.loc[market_train_df['time'] >= '2010-01-01 22:00:00+0000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9c921f8243bbcb70223cb5d931d19a2c6191e5a5"
   },
   "source": [
    "<h1>Initialize and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b552b6f1fbac2a1e78fdafd0612c08ce8684357b"
   },
   "outputs": [],
   "source": [
    "model = model('lgbm_71')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "165a4b006519272bd71db59e430782e6e48eaabd"
   },
   "outputs": [],
   "source": [
    "target = market_train_df.returnsOpenNextMktres10\n",
    "market_train_df.drop('returnsOpenNextMktres10', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0277beff74324c769a7ea4d7ce59553873204f86"
   },
   "outputs": [],
   "source": [
    "model.train([market_train_df, news_train_df], target, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c102bb7bdc0a399341d588039a62449d01b76391"
   },
   "outputs": [],
   "source": [
    "model.inspect(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>(load pre-trained models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33a05f3a4f3d6e20ca59fc49e4c2c3b3029254bd"
   },
   "source": [
    "<h1>Prediction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "32bf42d86ee9078a6810cd07fcd5a353ccfe40f7"
   },
   "outputs": [],
   "source": [
    "days = env.get_prediction_days()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "57d93926e20010685ff3e708258b04b6d529a2ea"
   },
   "outputs": [],
   "source": [
    "# skip a prediction (for testing)\n",
    "#env.predict(predictions_template_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ef60bc52a8a228e5a2ce18e4bd416f1f1f25aeae"
   },
   "outputs": [],
   "source": [
    "from time import time, ctime\n",
    "\n",
    "total_market_df = pd.DataFrame(columns=['time', 'assetCode', 'assetName', 'volume', 'close', 'open',\n",
    "       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n",
    "       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n",
    "       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n",
    "       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10','period', 'universe'])\n",
    "\n",
    "max_lag, days_count = model.max_lag, 0\n",
    "for (market_obs_df, news_obs_df, predictions_template_df) in days:\n",
    "    days_count += 1\n",
    "    if not days_count%50: print(days_count)\n",
    "        \n",
    "    market_obs_df['period']   = days_count\n",
    "    market_obs_df['universe'] = 1\n",
    "    \n",
    "    start_time = time()\n",
    "    total_market_df = pd.concat([total_market_df, market_obs_df])\n",
    "    #total_news_obs_df.append(news_obs_df)\n",
    "        \n",
    "    history_market_df = total_market_df[total_market_df['period'] > days_count - max_lag - 1].drop('period', axis=1).reset_index(drop=True)\n",
    "    confidence = model.predict_rolling([history_market_df, None],\n",
    "                                        len(predictions_template_df), verbose=True)\n",
    "    \n",
    "    predictions_template_df.confidenceValue = confidence\n",
    "    env.predict(predictions_template_df)\n",
    "    print(\"[{}] loop prediction {}, TIME {}\".format(days_count, ctime(), time()-start_time))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c8fbcca87c7f6abc53e86408417bf12ce21bb7f"
   },
   "source": [
    "## **`write_submission_file`** function\n",
    "\n",
    "Writes your predictions to a CSV file (`submission.csv`) in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c8ed34ffb2c47c6e124530ec798c0b4eb01ddd5"
   },
   "outputs": [],
   "source": [
    "env.write_submission_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d38aa8a67cad3f0c105db7e764ec9b805db39ceb"
   },
   "outputs": [],
   "source": [
    "# We've got a submission file!\n",
    "import os\n",
    "print([filename for filename in os.listdir('.') if '.csv' in filename])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f464f37885ffa763a2592e2867d74685f75be506"
   },
   "source": [
    "As indicated by the helper message, calling `write_submission_file` on its own does **not** make a submission to the competition.  It merely tells the module to write the `submission.csv` file as part of the Kernel's output.  To make a submission to the competition, you'll have to **Commit** your Kernel and find the generated `submission.csv` file in that Kernel Version's Output tab (note this is _outside_ of the Kernel Editor), then click \"Submit to Competition\".  When we re-run your Kernel during Stage Two, we will run the Kernel Version (generated when you hit \"Commit\") linked to your chosen Submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e3a267ea3149403c49ff59515a1a669ca2d1f9f"
   },
   "source": [
    "## Restart the Kernel to run your code again\n",
    "In order to combat cheating, you are only allowed to call `make_env` or iterate through `get_prediction_days` once per Kernel run.  However, while you're iterating on your model it's reasonable to try something out, change the model a bit, and try it again.  Unfortunately, if you try to simply re-run the code, or even refresh the browser page, you'll still be running on the same Kernel execution session you had been running before, and the `twosigmanews` module will still throw errors.  To get around this, you need to explicitly restart your Kernel execution session, which you can do by pressing the Restart button in the Kernel Editor's bottom Console tab:\n",
    "![Restart button](https://i.imgur.com/hudu8jF.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
