{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores 0.704"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fadffcaa4012badddbe0acd50c7b57b0f65843f2"
   },
   "source": [
    "<h1>Single model submission template.\n",
    "\n",
    "<p>every model to be used must satisfy the APIs requirements of model_template.py, and must pass all the tests like test_model_template.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2afca6b204da170ff55f3037a9f1bfda05c0a7c4"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a template for the APIs of models to be used into the stacking framework.\n",
    "run with Python 3.x\n",
    "\"\"\"\n",
    "from time import time, ctime\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "from matplotlib import pyplot as plt\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "from datetime import datetime, date\n",
    "import shap\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "def sigma_score(preds, valid_data):\n",
    "    \"\"\"\n",
    "    this is a custom metric used to train the model_lgbm_baseline\n",
    "    \"\"\"\n",
    "    df_time = valid_data.params['extra_time'] # will be injected afterwards\n",
    "    labels = valid_data.get_label()\n",
    "\n",
    "    #    assert len(labels) == len(df_time)\n",
    "\n",
    "    x_t = preds * labels #  * df_valid['universe'] -> Here we take out the 'universe' term because we already keep only those equals to 1.\n",
    "\n",
    "    # Here we take advantage of the fact that `labels` (used to calculate `x_t`)\n",
    "    # is a pd.Series and call `group_by`\n",
    "    x_t_sum = x_t.groupby(df_time).sum()\n",
    "    score = x_t_sum.mean() / x_t_sum.std()\n",
    "\n",
    "    return 'sigma_score', score, True\n",
    "\n",
    "class model():\n",
    "    \"\"\"this is a baseline lightLGB model with simple features\n",
    "\n",
    "    this class is for a model (that can also be\n",
    "    a combination of bagged models)\n",
    "    The commonality of the bagged models is that\n",
    "    they share the feature generation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name             = name\n",
    "        self.type             = lgb.Booster\n",
    "        self.model1 = None\n",
    "        self.model2 = None\n",
    "        self.model3 = None\n",
    "        self.model4 = None\n",
    "        self.model5 = None\n",
    "        self.model6 = None\n",
    "        self.training_results = None\n",
    "        print(\"\\ninit model {}\".format(self.name))\n",
    "        sys.path.insert(0, '../') # this is for imports from /kernels\n",
    "\n",
    "    def _preprocess(self, market_data):\n",
    "        \"\"\"optional data preprocessing\"\"\"\n",
    "        try:\n",
    "            market_data = market_data.loc[market_data['time']>=date(2010, 1, 1)]\n",
    "        except TypeError: # if 'time' is a string value\n",
    "            print(\"[_generate_features] 'time' is of type str and not datetime\")\n",
    "            if not market_data.loc[market_data['time']>=\"2010\"].empty:\n",
    "                # if dates are before 2010 means dataset is for testing\n",
    "                market_data = market_data.loc[market_data['time']>=\"2010\"]\n",
    "        assert market_data.empty == False\n",
    "        return market_data\n",
    "\n",
    "\n",
    "    def _generate_features(self, market_data, news_data, verbose=False, normalize=True, normalize_vals=[None], output_len = None):\n",
    "        \"\"\"\n",
    "        GENERAL:\n",
    "        given the original market_data and news_data\n",
    "        generate new features, doesn't change original data.\n",
    "        NOTE: data cleaning and preprocessing is not here,\n",
    "        here is only feats engineering\n",
    "\n",
    "        MODEL SPECIFIC:\n",
    "        as as a baseline for decision trees model we add\n",
    "        features that are the most popular among public\n",
    "        kernels on Kaggle:\n",
    "\n",
    "        - [36] short-term lagged features on returns\n",
    "        - has been removed (cant pass tests) [6]  long-term moving averages\n",
    "        - [1]  day of the week\n",
    "\n",
    "        Args:\n",
    "            [market_train_df, news_train_df]: pandas.DataFrame\n",
    "            normalize: (bool)\n",
    "            normalize_vals: None or [maxs, mins], normalize with local vals or with given vals\n",
    "            unique_assetCodess: list(str),for mapping assetCodeT\n",
    "\n",
    "        Returns:\n",
    "            complete_features: pandas.DataFrame\n",
    "        \"\"\"\n",
    "        #from utils import progress\n",
    "        start_time = time()\n",
    "        if verbose: print(\"Starting features generation for model {}, {}\".format(self.name, ctime()))\n",
    "\n",
    "        complete_features = market_data.copy()\n",
    "\n",
    "        if 'returnsOpenNextMktres10' in complete_features.columns:\n",
    "            complete_features.drop(['returnsOpenNextMktres10'],axis=1,inplace=True)\n",
    "\n",
    "        #### [36] short-term lagged features on returns ####\n",
    "\n",
    "\n",
    "        def create_lag(df_code,n_lag=[3,7,14,],shift_size=1):\n",
    "            code = df_code['assetCode'].unique()\n",
    "\n",
    "            # how to print progress in preprocessing?\n",
    "            # progress(0, len(n_lag)*len(return_features), prefix = 'Lagged features generation:', length=50)\n",
    "            # print(\"\\rcreating lags for {}\".format(code))\n",
    "            for _feature, col in enumerate(return_features):\n",
    "                for _lag, window in enumerate(n_lag):\n",
    "                    rolled = df_code[col].shift(shift_size).rolling(window=window)\n",
    "                    lag_mean = rolled.mean()\n",
    "                    lag_max = rolled.max()\n",
    "                    lag_min = rolled.min()\n",
    "                    lag_std = rolled.std()\n",
    "                    df_code['lag_%s_%s_mean'%(window,col)] = lag_mean\n",
    "                    df_code['lag_%s_%s_max'%(window,col)] = lag_max\n",
    "                    df_code['lag_%s_%s_min'%(window,col)] = lag_min\n",
    "        #             df_code['%s_lag_%s_std'%(col,window)] = lag_std\n",
    "                    #progress(_feature * len(n_lag) + _lag, len(n_lag) * len(return_features),\n",
    "                    #prefix = 'Lagged features generation:', length = 50)\n",
    "            return df_code.fillna(-1)\n",
    "\n",
    "        def generate_lag_features(df,n_lag = [3,7,14]):\n",
    "            features = ['time', 'assetCode', 'assetName', 'volume', 'close', 'open',\n",
    "               'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n",
    "               'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n",
    "               'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n",
    "               'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n",
    "               'returnsOpenNextMktres10', 'universe']\n",
    "\n",
    "            assetCodes = df['assetCode'].unique()\n",
    "            print(assetCodes)\n",
    "            all_df = []\n",
    "            df_codes = df.groupby('assetCode')\n",
    "            df_codes = [df_code[1][['time','assetCode']+return_features] for df_code in df_codes]\n",
    "            print('total %s df'%len(df_codes))\n",
    "\n",
    "            pool = Pool(4)\n",
    "            all_df = pool.map(create_lag, df_codes)\n",
    "\n",
    "            new_df = pd.concat(all_df)\n",
    "            new_df.drop(return_features,axis=1,inplace=True)\n",
    "            pool.close()\n",
    "\n",
    "            # for the next two lines\n",
    "            # https://stackoverflow.com/questions/49888485/pathos-multiprocessings-pool-appears-to-be-nonlocal\n",
    "            pool.terminate()\n",
    "            pool.restart()\n",
    "\n",
    "            return new_df\n",
    "\n",
    "        return_features = ['returnsClosePrevMktres10','returnsClosePrevRaw10','open','close']\n",
    "        n_lag = [3,7,14]\n",
    "        new_df = generate_lag_features(complete_features,n_lag=n_lag)\n",
    "        new_df['time'] = pd.to_datetime(new_df['time'])\n",
    "        complete_features['time'] = pd.to_datetime(complete_features['time'])\n",
    "        complete_features = pd.merge(complete_features,new_df,how='left',on=['time','assetCode'])\n",
    "        self.max_lag = max(n_lag)\n",
    "\n",
    "        if output_len is not None:\n",
    "            complete_features = complete_features[-output_len:]\n",
    "\n",
    "        complete_features = self._clean_data(complete_features)\n",
    "\n",
    "        #### [1]  generate labels encoding for assetCode ####\n",
    "\n",
    "        def data_prep(market_train, unique_assetCodes):\n",
    "            \"\"\"procedure from https://www.kaggle.com/guowenrui/sigma-eda-versionnew\n",
    "            Args:\n",
    "                market_train: df\n",
    "                unique_assetCodes: market_train['assetCode'].unique() this should be standard map!\n",
    "            \"\"\"\n",
    "            lbl = {k: v for v, k in enumerate(unique_assetCodes)}\n",
    "            market_train['assetCodeT'] = market_train['assetCode'].map(lbl) # this might get an error because mapping doesn't exist (read below)\n",
    "\n",
    "            # so the mapping has a bug, I should always use the same map and not every time a different map\n",
    "            # I might get assetCode not in the map, in that case need to handle exception putting (len + 1) as mapping value\n",
    "\n",
    "\n",
    "            market_train = market_train.dropna(axis=0)\n",
    "            return market_train\n",
    "\n",
    "        complete_features = data_prep(complete_features, complete_features['assetCode'].unique())\n",
    "\n",
    "        #### drop columns ####\n",
    "\n",
    "        fcol = [c for c in complete_features if c not in ['assetCode', 'assetCodes', 'assetCodesLen', 'assetName', 'audiences',\n",
    "                                                         'firstCreated', 'headline', 'headlineTag', 'marketCommentary', 'provider',\n",
    "                                                                                                      'returnsOpenNextMktres10', 'sourceId', 'subjects', 'time', 'time_x', 'universe','sourceTimestamp']]\n",
    "        complete_features = complete_features[fcol]\n",
    "\n",
    "\n",
    "        #### normalization of input ####\n",
    "\n",
    "        if normalize:\n",
    "            if len(normalize_vals) == 1:\n",
    "                mins = np.min(complete_features, axis=0)\n",
    "                maxs = np.max(complete_features, axis=0)\n",
    "                self.mins = mins #saved for prediction phase\n",
    "                self.maxs = maxs #saved for prediction phase\n",
    "                rng = maxs - mins\n",
    "                complete_features = 1 - ((maxs - complete_features) / rng)\n",
    "            else:\n",
    "                mins = normalize_vals[1]\n",
    "                maxs = normalize_vals[0]\n",
    "                rng = maxs - mins\n",
    "                complete_features = 1 - ((maxs - complete_features) / rng)\n",
    "\n",
    "\n",
    "        if verbose: print(\"Finished features generation for model {}, TIME {}\".format(self.name, time()-start_time))\n",
    "        return complete_features\n",
    "\n",
    "    def _generate_target(self, Y):\n",
    "        \"\"\"\n",
    "        given Y generate binary labels\n",
    "        returns:\n",
    "            up, r : (binary labels), (returns)\n",
    "        \"\"\"\n",
    "        binary_labels = Y >= 0\n",
    "        return binary_labels.astype(int).values, Y.values\n",
    "\n",
    "    def train(self, X, Y, verbose=False, normalize=True, normalize_vals=[None]):\n",
    "        \"\"\"\n",
    "        GENERAL:\n",
    "        basic method to train a model with given data\n",
    "        model will be inside self.model after training\n",
    "\n",
    "        MODEL SPECIFIC:\n",
    "\n",
    "        - sklearn random split\n",
    "        - universe filter on validation\n",
    "        - binary classification\n",
    "            need to put 'metric':'None' in parameters\n",
    "        - target is Y > 0\n",
    "\n",
    "        Args:\n",
    "            X: [market_train_df, news_train_df]\n",
    "            Y: [target]\n",
    "            verbose: (bool)\n",
    "        Returns:\n",
    "            (optional) training_results\n",
    "        \"\"\"\n",
    "        start_time = time()\n",
    "        if verbose: print(\"Starting training for model {}, {}\".format(self.name, ctime()))\n",
    "\n",
    "        time_reference = X[0]['time'] #time is dropped in preprocessing, but is needed later for metrics eval\n",
    "        universe_reference = X[0]['universe']\n",
    "\n",
    "        X = self._generate_features(X[0], X[1], verbose=verbose, normalize=normalize, normalize_vals=normalize_vals)\n",
    "        binary_Y, Y = self._generate_target(Y)\n",
    "\n",
    "        try:\n",
    "            assert X.shape[0] == binary_Y.shape[0] == Y.shape[0]\n",
    "        except AssertionError:\n",
    "            import pdb;pdb.set_trace()\n",
    "            pass\n",
    "\n",
    "        from sklearn import model_selection\n",
    "        X_train, X_val,\\\n",
    "        binary_Y_train, binary_Y_val,\\\n",
    "        Y_train, Y_val,\\\n",
    "        universe_train, universe_val,\\\n",
    "        time_train, time_val = model_selection.train_test_split(\n",
    "                X,\n",
    "                binary_Y,\n",
    "                Y,\n",
    "                universe_reference.values,\n",
    "                time_reference, test_size=0.25, random_state=99)\n",
    "\n",
    "        assert X_train.shape[0] == Y_train.shape[0] == binary_Y_train.shape[0]\n",
    "\n",
    "        if verbose: print(\"X_train shape {}\".format(X_train.shape))\n",
    "        if verbose: print(\"X_val shape {}\".format(X_val.shape))\n",
    "        assert X_train.shape[0] != X_val.shape[0]\n",
    "        assert X_train.shape[1] == X_val.shape[1]\n",
    "\n",
    "        # train parameters prearation\n",
    "        train_cols = X.columns.tolist()\n",
    "        assert 'returnsOpenNextMktres10' not in train_cols\n",
    "        train_data = lgb.Dataset(X.values, binary_Y, feature_name=train_cols)\n",
    "        test_data = lgb.Dataset(X_val.values, binary_Y_val, feature_name=train_cols)\n",
    "\n",
    "        x_1 = [0.19000424246380565, 2452, 212, 328, 202]\n",
    "        x_2 = [0.19016805202090095, 2583, 213, 312, 220]\n",
    "        x_3 = [0.19564034613157152, 2452, 210, 160, 219]\n",
    "        x_4 = [0.19016805202090095, 2500, 213, 150, 202]\n",
    "        x_5 = [0.19000424246380565, 2600, 215, 140, 220]\n",
    "        x_6 = [0.19000424246380565, 2652, 216, 152, 202]\n",
    "\n",
    "        params_1 = {\n",
    "                'task': 'train',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'learning_rate': x_1[0],\n",
    "                'num_leaves': x_1[1],\n",
    "                'min_data_in_leaf': x_1[2],\n",
    "                'num_iteration': 239,\n",
    "                'max_bin': x_1[4],\n",
    "                'verbose': 1\n",
    "            }\n",
    "\n",
    "        params_2 = {\n",
    "                'task': 'train',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'learning_rate': x_2[0],\n",
    "                'num_leaves': x_2[1],\n",
    "                'min_data_in_leaf': x_2[2],\n",
    "                'num_iteration': 172,\n",
    "                'max_bin': x_2[4],\n",
    "                'verbose': 1\n",
    "            }\n",
    "\n",
    "\n",
    "        params_3 = {\n",
    "                'task': 'train',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'learning_rate': x_3[0],\n",
    "                'num_leaves': x_3[1],\n",
    "                'min_data_in_leaf': x_3[2],\n",
    "                'num_iteration': x_3[3],\n",
    "                'max_bin': x_3[4],\n",
    "                'verbose': 1\n",
    "            }\n",
    "\n",
    "        params_4 = {\n",
    "                'task': 'train',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'learning_rate': x_4[0],\n",
    "                'num_leaves': x_4[1],\n",
    "                'min_data_in_leaf': x_4[2],\n",
    "                'num_iteration': x_4[3],\n",
    "                'max_bin': x_4[4],\n",
    "                'verbose': 1\n",
    "            }\n",
    "\n",
    "        params_5 = {\n",
    "                'task': 'train',\n",
    "                'boosting_type': 'gbdt',#dart\n",
    "                'objective': 'binary',\n",
    "                'learning_rate': x_5[0],\n",
    "                'num_leaves': x_5[1],\n",
    "                'min_data_in_leaf': x_5[2],\n",
    "                'num_iteration': x_5[3],\n",
    "                'max_bin': x_5[4],\n",
    "                'verbose': 1\n",
    "            }\n",
    "\n",
    "        params_6 = {\n",
    "                'task': 'train',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'learning_rate': x_6[0],\n",
    "                'num_leaves': x_6[1],\n",
    "                'min_data_in_leaf': x_6[2],\n",
    "                'num_iteration': x_6[3],\n",
    "                'max_bin': x_6[4],\n",
    "                'verbose': 1\n",
    "            }\n",
    "\n",
    "        training_results = {}\n",
    "        self.model1 = lgb.train(params_1,\n",
    "                train_data,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=(test_data, train_data),\n",
    "                valid_names=('valid','train'),\n",
    "                early_stopping_rounds=5,\n",
    "                verbose_eval=1,\n",
    "                evals_result=training_results)\n",
    "\n",
    "        self.model2 = lgb.train(params_2,\n",
    "                train_data,\n",
    "                valid_sets=(test_data, train_data),\n",
    "                valid_names=('valid','train'),\n",
    "                num_boost_round=100,\n",
    "                verbose_eval=1,\n",
    "                early_stopping_rounds=5,\n",
    "                evals_result=training_results)\n",
    "\n",
    "\n",
    "        self.model3 = lgb.train(params_3,\n",
    "                train_data,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=test_data,\n",
    "                early_stopping_rounds=5,\n",
    "        #         fobj=exp_loss,\n",
    "                )\n",
    "\n",
    "        self.model4 = lgb.train(params_4,\n",
    "                train_data,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=test_data,\n",
    "                early_stopping_rounds=5,\n",
    "        #         fobj=exp_loss,\n",
    "                )\n",
    "\n",
    "        self.model5 = lgb.train(params_5,\n",
    "                train_data,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=test_data,\n",
    "                early_stopping_rounds=5,\n",
    "        #         fobj=exp_loss,\n",
    "                )\n",
    "\n",
    "\n",
    "        self.model6 = lgb.train(params_6,\n",
    "                train_data,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=test_data,\n",
    "                early_stopping_rounds=10,\n",
    "        #         fobj=exp_loss,\n",
    "                )\n",
    "\n",
    "        del X, X_train, X_val\n",
    "\n",
    "        if verbose: print(\"Finished training for model {}, TIME {}\".format(self.name, time()-start_time))\n",
    "        \n",
    "        try:\n",
    "            self._save()\n",
    "        except:\n",
    "            print(\"[train] WARNING: couldn't save the model!\")\n",
    "        self.training_results = training_results\n",
    "        return training_results\n",
    "\n",
    "    def predict(self, X, verbose=False, do_shap=False, normalize=True, normalize_vals = [None]):\n",
    "        \"\"\"\n",
    "        given a block of X features gives prediction for everyrow+\".pkl\"\n",
    "\n",
    "        Args:\n",
    "            X: [market_train_df, news_train_df]\n",
    "            shap: perform shap analysis\n",
    "            normalize: (bool)\n",
    "            normalize_vals: recommmended self.maxs, self.mins\n",
    "        Returns:\n",
    "            y: pandas.Series\n",
    "        \"\"\"\n",
    "        start_time = time()\n",
    "        if verbose: print(\"Starting prediction for model {}, {}\".format(self.name, ctime()))\n",
    "        if self.model1 is None or self.model2 is None:\n",
    "            raise \"Error: model is not trained!\"\n",
    "\n",
    "        X_test = self._generate_features(X[0], X[1], verbose=verbose, normalize=normalize, normalize_vals=normalize_vals)\n",
    "        if verbose: print(\"X_test shape {}\".format(X_test.shape))\n",
    "        preds= []\n",
    "        preds.append(self.model1.predict(X_test))\n",
    "        preds.append(self.model2.predict(X_test))\n",
    "        preds.append(self.model3.predict(X_test))\n",
    "        preds.append(self.model4.predict(X_test))\n",
    "        preds.append(self.model5.predict(X_test))\n",
    "        preds.append(self.model6.predict(X_test))\n",
    "        y_test = self._postprocess(preds)\n",
    "\n",
    "        if do_shap:\n",
    "            #import pdb;pdb.set_trace()\n",
    "            print(\"printing shap analysis..\")\n",
    "            explainer = shap.TreeExplainer(self.model1)\n",
    "            shap_values = explainer.shap_values(X_test)\n",
    "            shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "\n",
    "        if verbose: print(\"Finished prediction for model {}, TIME {}\".format(self.name, time()-start_time))\n",
    "        return y_test\n",
    "\n",
    "    def predict_rolling(self, historical_df, market_obs_df, verbose=False, normalize=True, normalize_vals=[None]):\n",
    "        \"\"\"\n",
    "        predict features from X, uses historical for (lagged) feature generation\n",
    "        to be used with rolling prediciton structure from competition\n",
    "\n",
    "        Args:\n",
    "            historical_df: [market_train_df, news_train_df]\n",
    "            market_obs_df: from rolling prediction generator\n",
    "        \"\"\"\n",
    "        start_time = time()\n",
    "        if verbose: print(\"Starting rolled prediction for model {}, {}\".format(self.name, ctime()))\n",
    "        if self.model1 is None or self.model2 is None:\n",
    "            raise \"Error: model is not trained!\"\n",
    "\n",
    "        X_test = self._generate_features(historical_df[0], historical_df[1], verbose=verbose, normalize=normalize, normalize_vals=normalize_vals, output_len=len(market_obs_df))\n",
    "        X_test.reset_index(drop=True,inplace=True)\n",
    "        if verbose: print(\"X_test shape {}\".format(X_test.shape))\n",
    "        preds= []\n",
    "        preds.append(self.model1.predict(X_test))\n",
    "        preds.append(self.model2.predict(X_test))\n",
    "        preds.append(self.model3.predict(X_test))\n",
    "        preds.append(self.model4.predict(X_test))\n",
    "        preds.append(self.model5.predict(X_test))\n",
    "        preds.append(self.model6.predict(X_test))\n",
    "        y_test = self._postprocess(preds)\n",
    "\n",
    "        if verbose: print(\"Finished rolled prediction for model {}, TIME {}\".format(self.name, time()-start_time))\n",
    "        return y_test\n",
    "\n",
    "    def inspect(self, X):\n",
    "        \"\"\"\n",
    "        visualize and examine the training of the model\n",
    "        Args:\n",
    "            X: for the shap values\n",
    "\n",
    "        MODEL SPECIFIC:\n",
    "        plots training results and feature importance\n",
    "        \"\"\"\n",
    "        if not self.training_results:\n",
    "            print(\"Error: No training results available\")\n",
    "        else:\n",
    "            print(\"printing training results..\")\n",
    "            for _label, key in self.training_results.items():\n",
    "                for label, result in key.items():\n",
    "                    plt.plot(result,label=_label+\" \"+label)\n",
    "            plt.title(\"Training results\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        if not self.model1:\n",
    "            print(\"Error: No model available\")\n",
    "        else:\n",
    "            print(\"printing feature importance..\")\n",
    "            f=lgb.plot_importance(self.model1)\n",
    "            f.figure.set_size_inches(10, 30)\n",
    "            plt.show()\n",
    "\n",
    "    def _postprocess(self, predictions):\n",
    "        \"\"\"\n",
    "        post processing of predictions\n",
    "\n",
    "        Args:\n",
    "            predictions: list(np.array) might be from\n",
    "                different models\n",
    "        Return:\n",
    "            predictions: np.array\n",
    "\n",
    "        MODEL SPECIFIC:\n",
    "        the postprocessing is needed to ensemble bagged\n",
    "        models and to map prediction interval from [0, 1]\n",
    "        to [-1, 1]\n",
    "        \"\"\"\n",
    "        y_test = sum(predictions)/len(predictions)\n",
    "        y_test = (y_test-y_test.min())/(y_test.max()-y_test.min())\n",
    "        y_test = y_test * 2 - 1\n",
    "        return y_test\n",
    "\n",
    "    def _clean_data(self, data):\n",
    "        \"\"\"\n",
    "        originally from function mis_impute in\n",
    "        https://www.kaggle.com/guowenrui/sigma-eda-versionnew\n",
    "\n",
    "        Args:\n",
    "            data: pd.DataFrame\n",
    "        returns:\n",
    "            cleaned data (not in place)\n",
    "        \"\"\"\n",
    "        for i in data.columns:\n",
    "            if data[i].dtype == \"object\":\n",
    "                    data[i] = data[i].fillna(\"other\")\n",
    "            elif (data[i].dtype == \"int64\" or data[i].dtype == \"float64\"):\n",
    "                    data[i] = data[i].fillna(data[i].mean())\n",
    "                    # I am just filling the mean of all stocks together?\n",
    "                    # should fill with the mean of the singular stock\n",
    "            else:\n",
    "                    pass\n",
    "        return data\n",
    "\n",
    "    def _save(self):\n",
    "        \"\"\"\n",
    "        save models to memory into pickle/self.name\n",
    "        \"\"\"\n",
    "        to_save = [self.model1, self.model2, self.model3, self.model4, self.model5, self.model6]\n",
    "        if not all(to_save):\n",
    "            print(\"[_save] Error: not all models are trained\")\n",
    "            print(to_save)\n",
    "        else:\n",
    "            save_name = os.path.join(\"../pickle\",self.name+\"_\")\n",
    "            with open(save_name,\"wb\") as f:\n",
    "                pk.dump(to_save, f)\n",
    "                print(\"[_save] saved models to \"+save_name)\n",
    "\n",
    "    def _load(self):\n",
    "        \"\"\"\n",
    "        load models to memory from pickle/self.name\n",
    "        \"\"\"\n",
    "        save_name = os.path.join(\"../pickle\",self.name)+\".pkl\"\n",
    "        with open(save_name,\"rb\") as f:\n",
    "            models = pk.load(f)\n",
    "        self.model1 = models[0]\n",
    "        self.model2 = models[1]\n",
    "        self.model3 = models[2]\n",
    "        self.model4 = models[3]\n",
    "        self.model5 = models[4]\n",
    "        self.model6 = models[5]\n",
    "        print(\"[_load] models loaded succesfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "83cc460d930508a9445170726057d4ef07c254c9"
   },
   "source": [
    "<h1>Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from kaggle.competitions import twosigmanews\n",
    "# You can only call make_env() once, so don't lose it!\n",
    "env = twosigmanews.make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c20fa6deeac9d374c98774abd90bdc76b023ee63"
   },
   "outputs": [],
   "source": [
    "(market_train_df, news_train_df) = env.get_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "411b70914babbfa97b11a7d72409d92de936d721"
   },
   "source": [
    "<h1>`Datacleaning and preprocessing procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea1d2106702907555a9794d4194b4a23915b5ead"
   },
   "source": [
    "datacleaning will applied to the whole dataset for every model, the only requirements is that at the end of the procedure ***NO NEW FEATURES can be added here***. They must be added inside the feature generation section of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dc6b6b18ea855a1ac6a7b406a2053cd10ebc3493"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2531071b01eae4a4451eb5c9a8a494adebdeaa05"
   },
   "outputs": [],
   "source": [
    "market_train_df = market_train_df.loc[market_train_df['time'] >= '2010-01-01 22:00:00+0000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9c921f8243bbcb70223cb5d931d19a2c6191e5a5"
   },
   "source": [
    "<h1>Initialize and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b552b6f1fbac2a1e78fdafd0612c08ce8684357b"
   },
   "outputs": [],
   "source": [
    "model = model('lgbm_71_leak')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "165a4b006519272bd71db59e430782e6e48eaabd"
   },
   "outputs": [],
   "source": [
    "target = market_train_df.returnsOpenNextMktres10\n",
    "market_train_df.drop('returnsOpenNextMktres10', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0277beff74324c769a7ea4d7ce59553873204f86"
   },
   "outputs": [],
   "source": [
    "model.train([market_train_df, news_train_df], target, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2ef8574c1c8fd00d1ea74680cf34b67a35f0b70d"
   },
   "outputs": [],
   "source": [
    "max_values, min_values, max_lag = model.maxs, model.mins, model.max_lag # values used for normalization during predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c102bb7bdc0a399341d588039a62449d01b76391"
   },
   "outputs": [],
   "source": [
    "model.inspect(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33a05f3a4f3d6e20ca59fc49e4c2c3b3029254bd"
   },
   "source": [
    "<h1>Prediction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ddd50a7872c8c40d66429c2ff988bb3b827f7e3f"
   },
   "outputs": [],
   "source": [
    "days = env.get_prediction_days()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ef60bc52a8a228e5a2ce18e4bd416f1f1f25aeae"
   },
   "outputs": [],
   "source": [
    "\"\"\"locals required\n",
    "model: instance of model class defined above\n",
    "max_values, min_values: (pd.DataFrame)\n",
    "max_lag: (int)\n",
    "\"\"\"\n",
    "from time import time\n",
    "n_days, prep_time, prediction_time, packaging_time = 0, 0, 0, 0\n",
    "total_market_obs_df = []\n",
    "for (market_obs_df, news_obs_df, predictions_template_df) in days:\n",
    "    n_days +=1\n",
    "    if (n_days%50==0): print(n_days,end=' ')\n",
    "    t = time()\n",
    "    market_obs_df['time'] = market_obs_df['time'].dt.date\n",
    "\n",
    "    total_market_obs_df.append(market_obs_df)\n",
    "    if len(total_market_obs_df) == 1:\n",
    "        history_df = total_market_obs_df[0]\n",
    "    else:\n",
    "        history_df = pd.concat(total_market_obs_df[-(max_lag + 1):])\n",
    "        \n",
    "    confidence = model.predict_rolling([history_df, None], market_obs_df, verbose=True, normalize=True, normalize_vals = [max_values, min_values])      \n",
    "        \n",
    "    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':confidence})\n",
    "    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n",
    "    env.predict(predictions_template_df)\n",
    "    packaging_time += time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e996e8d31a1d28fced248f48c1874df2568cd29b"
   },
   "outputs": [],
   "source": [
    "history_market_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c8fbcca87c7f6abc53e86408417bf12ce21bb7f"
   },
   "source": [
    "## **`write_submission_file`** function\n",
    "\n",
    "Writes your predictions to a CSV file (`submission.csv`) in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c8ed34ffb2c47c6e124530ec798c0b4eb01ddd5"
   },
   "outputs": [],
   "source": [
    "env.write_submission_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d38aa8a67cad3f0c105db7e764ec9b805db39ceb"
   },
   "outputs": [],
   "source": [
    "# We've got a submission file!\n",
    "import os\n",
    "print([filename for filename in os.listdir('.') if '.csv' in filename])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f464f37885ffa763a2592e2867d74685f75be506"
   },
   "source": [
    "As indicated by the helper message, calling `write_submission_file` on its own does **not** make a submission to the competition.  It merely tells the module to write the `submission.csv` file as part of the Kernel's output.  To make a submission to the competition, you'll have to **Commit** your Kernel and find the generated `submission.csv` file in that Kernel Version's Output tab (note this is _outside_ of the Kernel Editor), then click \"Submit to Competition\".  When we re-run your Kernel during Stage Two, we will run the Kernel Version (generated when you hit \"Commit\") linked to your chosen Submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e3a267ea3149403c49ff59515a1a669ca2d1f9f"
   },
   "source": [
    "## Restart the Kernel to run your code again\n",
    "In order to combat cheating, you are only allowed to call `make_env` or iterate through `get_prediction_days` once per Kernel run.  However, while you're iterating on your model it's reasonable to try something out, change the model a bit, and try it again.  Unfortunately, if you try to simply re-run the code, or even refresh the browser page, you'll still be running on the same Kernel execution session you had been running before, and the `twosigmanews` module will still throw errors.  To get around this, you need to explicitly restart your Kernel execution session, which you can do by pressing the Restart button in the Kernel Editor's bottom Console tab:\n",
    "![Restart button](https://i.imgur.com/hudu8jF.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
