{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'market_test_df.csv', 'market_train_df.csv', 'market_train_df_head.csv', 'news_train_df_head.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../../data\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/Desktop/Coding/AI/two-sigma-kaggle/env3/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "#from kaggle.competitions import twosigmanews\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime, date\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b937b0dd28836dd738f2f58b2679730c2c5ca471"
   },
   "outputs": [],
   "source": [
    "# official way to get the data\n",
    "#from kaggle.competitions import twosigmanews\n",
    "#env = twosigmanews.make_env()\n",
    "#print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "b6bbecedd155b22dad4f89f21ecdeab8d9bce6a0"
   },
   "outputs": [],
   "source": [
    "market_train_df = pd.read_csv(\"../../data/market_train_df.csv\").drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "4e43901ec3febcf592da9c5a7a95df4d0d150309"
   },
   "outputs": [],
   "source": [
    "market_train_df['time'] = pd.to_datetime(market_train_df['time']).dt.date\n",
    "market_train_df = market_train_df.loc[market_train_df['time']>=date(2010, 1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "e5b830876a430d47175c03332ce9fc4ae4ba5aaa"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def create_lag(df_code,n_lag=[3,7,14,],shift_size=1):\n",
    "    code = df_code['assetCode'].unique()\n",
    "    \n",
    "    for col in return_features:\n",
    "        for window in n_lag:\n",
    "            rolled = df_code[col].shift(shift_size).rolling(window=window)\n",
    "            lag_mean = rolled.mean()\n",
    "            lag_max = rolled.max()\n",
    "            lag_min = rolled.min()\n",
    "            lag_std = rolled.std()\n",
    "            df_code['%s_lag_%s_mean'%(col,window)] = lag_mean\n",
    "            df_code['%s_lag_%s_max'%(col,window)] = lag_max\n",
    "            df_code['%s_lag_%s_min'%(col,window)] = lag_min\n",
    "#             df_code['%s_lag_%s_std'%(col,window)] = lag_std\n",
    "    return df_code.fillna(-1)\n",
    "\n",
    "def generate_lag_features(df,n_lag = [3,7,14]):\n",
    "    features = ['time', 'assetCode', 'assetName', 'volume', 'close', 'open',\n",
    "       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n",
    "       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n",
    "       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n",
    "       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n",
    "       'returnsOpenNextMktres10', 'universe']\n",
    "    \n",
    "    assetCodes = df['assetCode'].unique()\n",
    "    print(assetCodes)\n",
    "    all_df = []\n",
    "    df_codes = df.groupby('assetCode')\n",
    "    df_codes = [df_code[1][['time','assetCode']+return_features] for df_code in df_codes]\n",
    "    print('total %s df'%len(df_codes))\n",
    "    \n",
    "    pool = Pool(4)\n",
    "    all_df = pool.map(create_lag, df_codes)\n",
    "    \n",
    "    new_df = pd.concat(all_df)  \n",
    "    new_df.drop(return_features,axis=1,inplace=True)\n",
    "    pool.close()\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "0f71f40c0fb2fb6ca417bb419f0dd4c1b7efade2"
   },
   "outputs": [],
   "source": [
    "# return_features = ['close']\n",
    "# new_df = generate_lag_features(market_train_df,n_lag = 5)\n",
    "# market_train_df = pd.merge(market_train_df,new_df,how='left',on=['time','assetCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "169a93ab4e89f1dde12b56bd1e5d21b3c74e83b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A.N' 'AAI.N' 'AAP.N' ... 'HASI.N' 'NPTN.N' 'APFH.N']\n",
      "total 3274 df\n"
     ]
    }
   ],
   "source": [
    "return_features = ['returnsClosePrevMktres10','returnsClosePrevRaw10','open','close']\n",
    "n_lag = [3,7,14]\n",
    "new_df = generate_lag_features(market_train_df,n_lag=n_lag)\n",
    "market_train_df = pd.merge(market_train_df,new_df,how='left',on=['time','assetCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "5a661af9a09b648740faedea2ceffdd01c973b7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['time', 'assetCode', 'assetName', 'volume', 'close', 'open',\n",
      "       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n",
      "       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n",
      "       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n",
      "       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n",
      "       'returnsOpenNextMktres10', 'universe',\n",
      "       'returnsClosePrevMktres10_lag_3_mean',\n",
      "       'returnsClosePrevMktres10_lag_3_max',\n",
      "       'returnsClosePrevMktres10_lag_3_min',\n",
      "       'returnsClosePrevMktres10_lag_7_mean',\n",
      "       'returnsClosePrevMktres10_lag_7_max',\n",
      "       'returnsClosePrevMktres10_lag_7_min',\n",
      "       'returnsClosePrevMktres10_lag_14_mean',\n",
      "       'returnsClosePrevMktres10_lag_14_max',\n",
      "       'returnsClosePrevMktres10_lag_14_min',\n",
      "       'returnsClosePrevRaw10_lag_3_mean', 'returnsClosePrevRaw10_lag_3_max',\n",
      "       'returnsClosePrevRaw10_lag_3_min', 'returnsClosePrevRaw10_lag_7_mean',\n",
      "       'returnsClosePrevRaw10_lag_7_max', 'returnsClosePrevRaw10_lag_7_min',\n",
      "       'returnsClosePrevRaw10_lag_14_mean', 'returnsClosePrevRaw10_lag_14_max',\n",
      "       'returnsClosePrevRaw10_lag_14_min', 'open_lag_3_mean', 'open_lag_3_max',\n",
      "       'open_lag_3_min', 'open_lag_7_mean', 'open_lag_7_max', 'open_lag_7_min',\n",
      "       'open_lag_14_mean', 'open_lag_14_max', 'open_lag_14_min',\n",
      "       'close_lag_3_mean', 'close_lag_3_max', 'close_lag_3_min',\n",
      "       'close_lag_7_mean', 'close_lag_7_max', 'close_lag_7_min',\n",
      "       'close_lag_14_mean', 'close_lag_14_max', 'close_lag_14_min'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(market_train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "3a4a30d46a777b1fa5707fa869835e7cc77f9740"
   },
   "outputs": [],
   "source": [
    "# return_features = ['open']\n",
    "# new_df = generate_lag_features(market_train_df,n_lag=[3,7,14])\n",
    "# market_train_df = pd.merge(market_train_df,new_df,how='left',on=['time','assetCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "ae9ab77b243941d71a3bf593d2d33a000ecc8fbb"
   },
   "outputs": [],
   "source": [
    "def mis_impute(data):\n",
    "    for i in data.columns:\n",
    "        if data[i].dtype == \"object\":\n",
    "            data[i] = data[i].fillna(\"other\")\n",
    "        elif (data[i].dtype == \"int64\" or data[i].dtype == \"float64\"):\n",
    "            data[i] = data[i].fillna(data[i].mean())\n",
    "        else:\n",
    "            pass\n",
    "    return data\n",
    "\n",
    "market_train_df = mis_impute(market_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "71d125e8889f8bb3bd86f2e9c09b3527bad5d413"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2846738, 53)\n"
     ]
    }
   ],
   "source": [
    "def data_prep(market_train):\n",
    "    lbl = {k: v for v, k in enumerate(market_train['assetCode'].unique())}\n",
    "    market_train['assetCodeT'] = market_train['assetCode'].map(lbl)\n",
    "    market_train = market_train.dropna(axis=0)\n",
    "    return market_train\n",
    "\n",
    "market_train_df = data_prep(market_train_df)\n",
    "# # check the shape\n",
    "print(market_train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "be503073e88d79d4ab2a55ccffa01494ce6e19bb"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "up = market_train_df['returnsOpenNextMktres10'] >= 0\n",
    "\n",
    "\n",
    "universe = market_train_df['universe'].values\n",
    "d = market_train_df['time']\n",
    "\n",
    "fcol = [c for c in market_train_df if c not in ['assetCode', 'assetCodes', 'assetCodesLen', 'assetName', 'audiences', \n",
    "                                             'firstCreated', 'headline', 'headlineTag', 'marketCommentary', 'provider', \n",
    "                                             'returnsOpenNextMktres10', 'sourceId', 'subjects', 'time', 'time_x', 'universe','sourceTimestamp']]\n",
    "\n",
    "X = market_train_df[fcol].values\n",
    "up = up.values\n",
    "r = market_train_df.returnsOpenNextMktres10.values\n",
    "\n",
    "# Scaling of X values\n",
    "# It is good to keep these scaling values for later\n",
    "mins = np.min(X, axis=0)\n",
    "maxs = np.max(X, axis=0)\n",
    "rng = maxs - mins\n",
    "X = 1 - ((maxs - X) / rng)\n",
    "\n",
    "# Sanity check\n",
    "assert X.shape[0] == up.shape[0] == r.shape[0]\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "\n",
    "import pickle as pk\n",
    "with open(\"../pickle/_ref_market_train_df.pkl\",\"wb\") as f:\n",
    "    #dumps generated features for comparison\n",
    "    pk.dump(X, f)\n",
    "\n",
    "X_train, X_test, up_train, up_test, r_train, r_test,u_train,u_test,d_train,d_test = model_selection.train_test_split(X, up, r,universe,d, test_size=0.25, random_state=99)\n",
    "\n",
    "\n",
    "# te = market_train_df['time']>date(2015, 1, 1)\n",
    "\n",
    "# tt = 0\n",
    "# for tt,i in enumerate(te.values):\n",
    "#     if i:\n",
    "#         idx = tt\n",
    "#         print(i,tt)\n",
    "#         break\n",
    "# print(idx)\n",
    "# # for ind_tr, ind_te in tscv.split(X):\n",
    "# #     print(ind_tr)\n",
    "# X_train, X_test = X[:idx],X[idx:]\n",
    "\n",
    "# up_train, up_test = up[:idx],up[idx:]\n",
    "# r_train, r_test = r[:idx],r[idx:]\n",
    "# u_train,u_test = universe[:idx],universe[idx:]\n",
    "# d_train,d_test = d[:idx],d[idx:]\n",
    "\n",
    "# train_data = lgb.Dataset(X_train, label=up_train.astype(int))\n",
    "train_data = lgb.Dataset(X, label=up.astype(int))\n",
    "test_data = lgb.Dataset(X_test, label=up_test.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b90cf8057f62d6ed2263ec441889e9625e3dbbfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True False ... False False  True]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/Desktop/Coding/AI/two-sigma-kaggle/env3/lib/python3.7/site-packages/lightgbm/engine.py:116: UserWarning: Found `num_iteration` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.686931\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's binary_logloss: 0.682219\n",
      "[3]\tvalid_0's binary_logloss: 0.678278\n",
      "[4]\tvalid_0's binary_logloss: 0.674929\n",
      "[5]\tvalid_0's binary_logloss: 0.67202\n",
      "[6]\tvalid_0's binary_logloss: 0.66939\n",
      "[7]\tvalid_0's binary_logloss: 0.667001\n",
      "[8]\tvalid_0's binary_logloss: 0.664809\n",
      "[9]\tvalid_0's binary_logloss: 0.662691\n",
      "[10]\tvalid_0's binary_logloss: 0.660414\n",
      "[11]\tvalid_0's binary_logloss: 0.658421\n",
      "[12]\tvalid_0's binary_logloss: 0.656347\n",
      "[13]\tvalid_0's binary_logloss: 0.654358\n",
      "[14]\tvalid_0's binary_logloss: 0.652363\n",
      "[15]\tvalid_0's binary_logloss: 0.650411\n",
      "[16]\tvalid_0's binary_logloss: 0.648334\n",
      "[17]\tvalid_0's binary_logloss: 0.646779\n",
      "[18]\tvalid_0's binary_logloss: 0.644703\n",
      "[19]\tvalid_0's binary_logloss: 0.643046\n",
      "[20]\tvalid_0's binary_logloss: 0.640843\n",
      "[21]\tvalid_0's binary_logloss: 0.639291\n",
      "[22]\tvalid_0's binary_logloss: 0.63769\n",
      "[23]\tvalid_0's binary_logloss: 0.634964\n",
      "[24]\tvalid_0's binary_logloss: 0.633487\n",
      "[25]\tvalid_0's binary_logloss: 0.63204\n",
      "[26]\tvalid_0's binary_logloss: 0.630691\n",
      "[27]\tvalid_0's binary_logloss: 0.628801\n",
      "[28]\tvalid_0's binary_logloss: 0.627248\n",
      "[29]\tvalid_0's binary_logloss: 0.625206\n",
      "[30]\tvalid_0's binary_logloss: 0.623757\n",
      "[31]\tvalid_0's binary_logloss: 0.621385\n",
      "[32]\tvalid_0's binary_logloss: 0.620036\n",
      "[33]\tvalid_0's binary_logloss: 0.618735\n",
      "[34]\tvalid_0's binary_logloss: 0.61746\n",
      "[35]\tvalid_0's binary_logloss: 0.615754\n",
      "[36]\tvalid_0's binary_logloss: 0.614614\n",
      "[37]\tvalid_0's binary_logloss: 0.613146\n",
      "[38]\tvalid_0's binary_logloss: 0.611538\n",
      "[39]\tvalid_0's binary_logloss: 0.610414\n",
      "[40]\tvalid_0's binary_logloss: 0.60875\n",
      "[41]\tvalid_0's binary_logloss: 0.607684\n",
      "[42]\tvalid_0's binary_logloss: 0.606324\n",
      "[43]\tvalid_0's binary_logloss: 0.604975\n",
      "[44]\tvalid_0's binary_logloss: 0.603557\n",
      "[45]\tvalid_0's binary_logloss: 0.602286\n",
      "[46]\tvalid_0's binary_logloss: 0.600932\n",
      "[47]\tvalid_0's binary_logloss: 0.599895\n",
      "[48]\tvalid_0's binary_logloss: 0.597765\n",
      "[49]\tvalid_0's binary_logloss: 0.596661\n",
      "[50]\tvalid_0's binary_logloss: 0.595517\n",
      "[51]\tvalid_0's binary_logloss: 0.594054\n",
      "[52]\tvalid_0's binary_logloss: 0.592279\n",
      "[53]\tvalid_0's binary_logloss: 0.591254\n",
      "[54]\tvalid_0's binary_logloss: 0.589292\n",
      "[55]\tvalid_0's binary_logloss: 0.588334\n",
      "[56]\tvalid_0's binary_logloss: 0.587165\n",
      "[57]\tvalid_0's binary_logloss: 0.586137\n",
      "[58]\tvalid_0's binary_logloss: 0.584966\n",
      "[59]\tvalid_0's binary_logloss: 0.583855\n",
      "[60]\tvalid_0's binary_logloss: 0.582508\n",
      "[61]\tvalid_0's binary_logloss: 0.581157\n",
      "[62]\tvalid_0's binary_logloss: 0.579346\n",
      "[63]\tvalid_0's binary_logloss: 0.578303\n",
      "[64]\tvalid_0's binary_logloss: 0.577254\n",
      "[65]\tvalid_0's binary_logloss: 0.575675\n",
      "[66]\tvalid_0's binary_logloss: 0.574701\n",
      "[67]\tvalid_0's binary_logloss: 0.573395\n",
      "[68]\tvalid_0's binary_logloss: 0.572257\n",
      "[69]\tvalid_0's binary_logloss: 0.571237\n",
      "[70]\tvalid_0's binary_logloss: 0.570135\n",
      "[71]\tvalid_0's binary_logloss: 0.568493\n",
      "[72]\tvalid_0's binary_logloss: 0.567444\n",
      "[73]\tvalid_0's binary_logloss: 0.566287\n",
      "[74]\tvalid_0's binary_logloss: 0.565275\n",
      "[75]\tvalid_0's binary_logloss: 0.563739\n",
      "[76]\tvalid_0's binary_logloss: 0.562598\n",
      "[77]\tvalid_0's binary_logloss: 0.561369\n",
      "[78]\tvalid_0's binary_logloss: 0.560201\n",
      "[79]\tvalid_0's binary_logloss: 0.559231\n",
      "[80]\tvalid_0's binary_logloss: 0.557984\n",
      "[81]\tvalid_0's binary_logloss: 0.556222\n",
      "[82]\tvalid_0's binary_logloss: 0.555228\n",
      "[83]\tvalid_0's binary_logloss: 0.554258\n",
      "[84]\tvalid_0's binary_logloss: 0.553164\n",
      "[85]\tvalid_0's binary_logloss: 0.552188\n",
      "[86]\tvalid_0's binary_logloss: 0.551242\n",
      "[87]\tvalid_0's binary_logloss: 0.549974\n",
      "[88]\tvalid_0's binary_logloss: 0.549008\n",
      "[89]\tvalid_0's binary_logloss: 0.548031\n",
      "[90]\tvalid_0's binary_logloss: 0.54685\n"
     ]
    }
   ],
   "source": [
    "# these are tuned params I found\n",
    "x_1 = [0.19000424246380565, 2452, 212, 239, 202]\n",
    "x_2 = [0.19016805202090095, 2583, 213, 172, 220]\n",
    "x_3 = [0.19564034613157152, 2452, 210, 160, 219]\n",
    "x_4 = [0.19016805202090095, 2500, 213, 150, 202]\n",
    "x_5 = [0.19000424246380565, 2600, 215, 140, 220]\n",
    "x_6 = [0.19000424246380565, 2652, 216, 152, 202]\n",
    "\n",
    "\"\"\"\n",
    "x_1 = [0.19000424246380565, 2452, 212, 328, 202]\n",
    "x_2 = [0.19016805202090095, 2583, 213, 312, 220]\n",
    "x_3 = [0.19564034613157152, 2455, 210, 330, 219]\n",
    "x_4 = [0.15000000000000000, 2600, 220, 340, 202]\n",
    "x_5 = [0.15000000000000000, 2600, 220, 330, 220]\n",
    "x_6 = [0.15000000000000000, 2400, 220, 340, 202]\n",
    "\"\"\"\n",
    "print(up_train)\n",
    "def exp_loss(p,y):\n",
    "    y = y.get_label()\n",
    "#     p = p.get_label()\n",
    "    grad = -y*(1.0-1.0/(1.0+np.exp(-y*p)))\n",
    "    hess = -(np.exp(y*p)*(y*p-1)-1)/((np.exp(y*p)+1)**2)\n",
    "    \n",
    "    return grad,hess\n",
    "\n",
    "params_1 = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "#         'objective': 'regression',\n",
    "        'learning_rate': x_1[0],\n",
    "        'num_leaves': x_1[1],\n",
    "        'min_data_in_leaf': x_1[2],\n",
    "#         'num_iteration': x_1[3],\n",
    "        'num_iteration': 239,\n",
    "        'max_bin': x_1[4],\n",
    "        'verbose': 1\n",
    "    }\n",
    "\n",
    "params_2 = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "#         'objective': 'regression',\n",
    "        'learning_rate': x_2[0],\n",
    "        'num_leaves': x_2[1],\n",
    "        'min_data_in_leaf': x_2[2],\n",
    "#         'num_iteration': x_2[3],\n",
    "        'num_iteration': 172,\n",
    "        'max_bin': x_2[4],\n",
    "        'verbose': 1\n",
    "    }\n",
    "\n",
    "params_3 = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'learning_rate': x_3[0],\n",
    "        'num_leaves': x_3[1],\n",
    "        'min_data_in_leaf': x_3[2],\n",
    "        'num_iteration': x_3[3],\n",
    "        'max_bin': x_3[4],\n",
    "        'verbose': 1\n",
    "    }\n",
    "\n",
    "params_4 = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'learning_rate': x_4[0],\n",
    "        'num_leaves': x_4[1],\n",
    "        'min_data_in_leaf': x_4[2],\n",
    "        'num_iteration': x_4[3],\n",
    "        'max_bin': x_4[4],\n",
    "        'verbose': 1\n",
    "    }\n",
    "\n",
    "params_5 = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',#dart\n",
    "        'objective': 'binary',\n",
    "        'learning_rate': x_5[0],\n",
    "        'num_leaves': x_5[1],\n",
    "        'min_data_in_leaf': x_5[2],\n",
    "        'num_iteration': x_5[3],\n",
    "        'max_bin': x_5[4],\n",
    "        'verbose': 1\n",
    "    }\n",
    "\n",
    "params_6 = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'learning_rate': x_6[0],\n",
    "        'num_leaves': x_6[1],\n",
    "        'min_data_in_leaf': x_6[2],\n",
    "        'num_iteration': x_6[3],\n",
    "        'max_bin': x_6[4],\n",
    "        'verbose': 1\n",
    "    }\n",
    "\n",
    "gbm_1 = lgb.train(params_1,\n",
    "        train_data,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=test_data,\n",
    "        early_stopping_rounds=5,\n",
    "#         fobj=exp_loss,\n",
    "        )\n",
    "\n",
    "gbm_2 = lgb.train(params_2,\n",
    "        train_data,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=test_data,\n",
    "        early_stopping_rounds=5,\n",
    "#         fobj=exp_loss,\n",
    "        )\n",
    "\n",
    "gbm_3 = lgb.train(params_3,\n",
    "        train_data,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=test_data,\n",
    "        early_stopping_rounds=5,\n",
    "#         fobj=exp_loss,\n",
    "        )\n",
    "\n",
    "gbm_4 = lgb.train(params_4,\n",
    "        train_data,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=test_data,\n",
    "        early_stopping_rounds=5,\n",
    "#         fobj=exp_loss,\n",
    "        )\n",
    "\n",
    "gbm_5 = lgb.train(params_5,\n",
    "        train_data,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=test_data,\n",
    "        early_stopping_rounds=5,\n",
    "#         fobj=exp_loss,\n",
    "        )\n",
    "\n",
    "\n",
    "gbm_6 = lgb.train(params_6,\n",
    "        train_data,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=test_data,\n",
    "        early_stopping_rounds=10,\n",
    "#         fobj=exp_loss,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a17e60177286707f921201d670dbde90df6b2587"
   },
   "outputs": [],
   "source": [
    "#实现零均值归一化操作\n",
    "confidence_test = (gbm_1.predict(X_test) + gbm_2.predict(X_test)+gbm_3.predict(X_test)+gbm_4.predict(X_test)+gbm_5.predict(X_test)+gbm_6.predict(X_test))/6\n",
    "confidence_test = (confidence_test-confidence_test.min())/(confidence_test.max()-confidence_test.min())\n",
    "#print(\"max_min\",max_min)\n",
    "#confidence_test=(confidence_test-confidence_test.mean())/(confidence_test.std())\n",
    "confidence_test = confidence_test*2-1\n",
    "print(max(confidence_test),min(confidence_test))\n",
    "\n",
    "# calculation of actual metric that is used to calculate final score\n",
    "r_test = r_test.clip(-1,1) # get rid of outliers. Where do they come from??\n",
    "x_t_i = confidence_test * r_test * u_test\n",
    "data = {'day' : d_test, 'x_t_i' : x_t_i}\n",
    "df = pd.DataFrame(data)\n",
    "x_t = df.groupby('day').sum().values.flatten()\n",
    "mean = np.mean(x_t)\n",
    "std = np.std(x_t)\n",
    "score_test = mean / std\n",
    "print(score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c1eb446f1381422af71d38b1766426e2bff3c827"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "del X_train,X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd47dcb5966d8923981f8961b29f033895146892"
   },
   "outputs": [],
   "source": [
    "#prediction\n",
    "days = env.get_prediction_days()\n",
    "n_days = 0\n",
    "prep_time = 0\n",
    "prediction_time = 0\n",
    "packaging_time = 0\n",
    "total_market_obs_df = []\n",
    "for (market_obs_df, news_obs_df, predictions_template_df) in days:\n",
    "    n_days +=1\n",
    "    if (n_days%50==0):\n",
    "        pass\n",
    "        #print(n_days,end=' ')\n",
    "    t = time.time()\n",
    "    market_obs_df['time'] = market_obs_df['time'].dt.date\n",
    "    \n",
    "    return_features = ['returnsClosePrevMktres10','returnsClosePrevRaw10','open','close']\n",
    "    total_market_obs_df.append(market_obs_df)\n",
    "    if len(total_market_obs_df)==1:\n",
    "        history_df = total_market_obs_df[0]\n",
    "    else:\n",
    "        history_df = pd.concat(total_market_obs_df[-(np.max(n_lag)+1):])\n",
    "    new_df = generate_lag_features(history_df,n_lag=[3,7,14])\n",
    "    market_obs_df = pd.merge(market_obs_df,new_df,how='left',on=['time','assetCode'])\n",
    "    market_obs_df = mis_impute(market_obs_df)\n",
    "    market_obs_df = data_prep(market_obs_df)\n",
    "    \n",
    "    X_live = market_obs_df[fcol].values\n",
    "    X_live = 1 - ((maxs - X_live) / rng)\n",
    "    prep_time += time.time() - t\n",
    "    t = time.time()\n",
    "    lp = (gbm_1.predict(X_live) + gbm_2.predict(X_live)+gbm_3.predict(X_live)+gbm_4.predict(X_live)+gbm_5.predict(X_live)+gbm_6.predict(X_live))/6\n",
    "    prediction_time += time.time() -t\n",
    "    t = time.time()\n",
    "    confidence = lp\n",
    "    confidence = (confidence-confidence.min())/(confidence.max()-confidence.min())\n",
    "    #print(\"max_min_predict\",max_min_predict)\n",
    "    confidence = confidence * 2 - 1\n",
    "    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':confidence})\n",
    "    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n",
    "    env.predict(predictions_template_df)\n",
    "    packaging_time += time.time() - t\n",
    "    \n",
    "env.write_submission_file()\n",
    "sub  = pd.read_csv(\"submission_versionnew.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6b241d099c2568517807adf690772f0c3a2778e4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "28ad76994675278184d044adbaf1c4849d69ff38"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6bfbdc1237c6c213218ceb46a8f2e8f951d0708c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c9b9758209fbe62616b0dfd4c529630eb85a5816"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a3abd7c11965a8a3d079fa55f72fb8bc77448cc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
